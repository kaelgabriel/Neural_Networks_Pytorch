{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T01:20:10.347850Z",
     "start_time": "2017-09-09T01:20:10.339850Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meu nome é: Gabriel Moraes Barros \n",
      "Meu RA é: 192801\n"
     ]
    }
   ],
   "source": [
    "print('Meu nome é: Gabriel Moraes Barros ')\n",
    "print('Meu RA é: 192801')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plot\n",
    "from IPython import display\n",
    "\n",
    "import sys\n",
    "import numpy as np\n",
    "import numpy.random as nr\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "\n",
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Convolution2D, MaxPooling2D\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from keras.optimizers import (SGD, \n",
    "                              RMSprop, \n",
    "                              Adam, \n",
    "                              Adadelta, \n",
    "                              Adagrad)\n",
    "\n",
    "print('Keras ', keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "nr.seed(20170603)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls ../utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sys.path.append('../utils')\n",
    "from my_keras_utilities import (get_available_gpus, \n",
    "                                load_model_and_history, \n",
    "                                save_model_and_history, \n",
    "                                TrainingPlotter, train_network)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testa se um modulo foi importado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'my_keras_utilities' in sys.modules\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "try:\n",
    "    train_network(model_week05, model_name, train_generator, validation_generator, **fit_params);\n",
    "except AttributeError:\n",
    "      print('nope')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T01:20:10.882645Z",
     "start_time": "2017-09-09T01:20:10.751750Z"
    }
   },
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "K.set_image_data_format('channels_first')\n",
    "K.set_floatx('float32')\n",
    "print('Backend:        {}'.format(K.backend()))\n",
    "print('Data format:    {}'.format(K.image_data_format()))\n",
    "!nvidia-smib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T01:20:11.258830Z",
     "start_time": "2017-09-09T01:20:11.118586Z"
    }
   },
   "outputs": [],
   "source": [
    "!ls ../Task\\ 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Função auxiliar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "class MyCb(TrainingPlotter):\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "\n",
    "def train_network(model, model_name, train_generator, validation_generator, \n",
    "                  train_steps=10, valid_steps=10, opt='rmsprop', nepochs=50, \n",
    "                  patience=50, reset=False, ploss=1.0):\n",
    "\n",
    "    do_plot = (ploss > 0.0)\n",
    "    \n",
    "    model_fn = model_name + '.model'\n",
    "    if reset and os.path.isfile(model_fn):\n",
    "        os.unlink(model_name + '.model')\n",
    "        \n",
    "    if not os.path.isfile(model_fn):\n",
    "        # initialize the optimizer and model\n",
    "        print(\"[INFO] compiling model...\")\n",
    "        model.compile(loss=\"binary_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])    \n",
    "\n",
    "        # History, checkpoint, earlystop, plot losses:\n",
    "        cb = [ModelCheckpoint(model_file, monitor='val_acc', verbose=0, save_best_only=True, mode='auto', period=1),\n",
    "        MyCb(n=1, filepath=model_name, patience=patience, plot_losses=do_plot), \n",
    "        ReduceLROnPlateau(monitor='val_loss', factor=0.9, patience=7, verbose=0, mode='auto', epsilon=0.00001, cooldown=0, min_lr=0)\n",
    "     ]\n",
    "        \n",
    "    else:\n",
    "        print(\"[INFO] loading model...\")\n",
    "        model, cb = load_model_and_history(model_name)\n",
    "        cb.patience = patience\n",
    "\n",
    "    past_epochs = cb[1].get_nepochs()\n",
    "    tr_epochs = nepochs - past_epochs\n",
    "    \n",
    "    if do_plot:\n",
    "        vv = 0\n",
    "        fig = plot.figure(figsize=(15,6))\n",
    "        plot.ylim(0.0, ploss)\n",
    "        plot.xlim(0, nepochs)\n",
    "        plot.grid(True)\n",
    "    else:\n",
    "        vv = 2\n",
    "\n",
    "    print(\"[INFO] training for {} epochs ...\".format(tr_epochs))\n",
    "    try:\n",
    "        model.fit_generator(train_generator, steps_per_epoch=train_steps,\n",
    "                            validation_data=validation_generator, validation_steps=valid_steps,\n",
    "                            epochs=nepochs, verbose=vv, callbacks=[cb[1]])\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    model, histo = load_model_and_history(model_name)\n",
    "    return model, cb\n",
    "\n",
    "\n",
    "def test_network(model_name, validation_generator, nb_validation_samples):\n",
    "    model, histo = load_model_and_history(model_name)\n",
    "    print('Model from epoch {}'.format(histo.best_epoch))\n",
    "    print(\"[INFO] evaluating in the test data set ...\")\n",
    "    loss, accuracy = model.evaluate_generator(validation_generator, nb_validation_samples)\n",
    "    print(\"\\n[INFO] accuracy on the test data set: {:.2f}% [{:.5f}]\".format(accuracy * 100, loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subindo o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T01:20:12.005955Z",
     "start_time": "2017-09-09T01:20:12.000245Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#auternar o comentário, se estiver no client ou no remote\n",
    "data = np.load('/etc/jupyterhub/ia368z_2s2017/datasets/cifar10-redux.npz')\n",
    "#data = np.load('../Task 5/cifar10-redux.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T01:20:12.265870Z",
     "start_time": "2017-09-09T01:20:12.244731Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = data['X_train']\n",
    "y_train = data['y_train']\n",
    "X_test = data['X_test']\n",
    "y_test = data['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T01:20:12.504461Z",
     "start_time": "2017-09-09T01:20:12.496212Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train.dtype, y_train.dtype, X_test.dtype, y_test.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separando o conjunto de treinamento em validação e treinamento, numa proporção 80/20 %\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p=np.random.permutation(len(X_train))\n",
    "percent_factor=0.85\n",
    "new_train_x = X_train[p]\n",
    "new_train_y = y_train[p]\n",
    "\n",
    "\n",
    "new_X_train = new_train_x[0:(np.floor(len(new_train_x)*percent_factor))]\n",
    "new_y_train = new_train_y[0:(np.floor(len(new_train_y)*percent_factor))]\n",
    "new_X_val = new_train_x[(np.ceil(len(new_train_x)*percent_factor)):]\n",
    "new_y_val = new_train_y[(np.ceil(len(new_train_y)*percent_factor)):]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T01:20:12.773731Z",
     "start_time": "2017-09-09T01:20:12.765869Z"
    }
   },
   "outputs": [],
   "source": [
    "print('X_train.shape',new_X_train.shape)\n",
    "print('y_train.shape',new_y_train.shape)\n",
    "print('X_val.shape',new_X_val.shape)\n",
    "print('y_val.shape',new_y_val.shape)\n",
    "print('y_test shape ',y_test.shape)\n",
    "print('X_test.shape:',X_test.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T01:20:13.107292Z",
     "start_time": "2017-09-09T01:20:13.102728Z"
    }
   },
   "outputs": [],
   "source": [
    "print('Número de diferentes classes',len(np.unique(y_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizando os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T01:20:13.816180Z",
     "start_time": "2017-09-09T01:20:13.799877Z"
    }
   },
   "outputs": [],
   "source": [
    "a=0\n",
    "print(np.mean(X_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T01:20:14.172398Z",
     "start_time": "2017-09-09T01:20:14.135961Z"
    }
   },
   "source": [
    "#Guaranteeing that it only runs once\n",
    "if (a==0):\n",
    "    X_test = X_test.astype('float32')\n",
    "    new_X_train = new_X_train.astype('float32')\n",
    "    new_X_val = new_X_val.astype('float32')\n",
    "    \n",
    "    new_X_val /= 255.\n",
    "    new_X_train /= 255.\n",
    "    X_test /= 255.\n",
    "    \n",
    "    a=1\n",
    "print(np.mean(new_X_train))\n",
    "print(np.mean(new_X_val))\n",
    "print(np.mean(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "\n",
    "## Transforma o vetor de labels para o formato de one-hot encoding.\n",
    "n_classes = 3\n",
    "y_train_oh = np_utils.to_categorical(new_y_train-3, n_classes)\n",
    "y_val_oh = np_utils.to_categorical(new_y_val-3, n_classes)\n",
    "y_test_oh = np_utils.to_categorical(y_test-3, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T01:20:14.948527Z",
     "start_time": "2017-09-09T01:20:14.942812Z"
    }
   },
   "outputs": [],
   "source": [
    "print(y_train_oh.shape)\n",
    "print(y_val_oh.shape)\n",
    "print(y_test_oh.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fazendo o data augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T01:20:15.670473Z",
     "start_time": "2017-09-09T01:20:15.664241Z"
    }
   },
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('new x train shape', new_X_train.shape)\n",
    "print('y train oh shape', y_train_oh.shape)\n",
    "\n",
    "print('new x val shape', new_X_val.shape)\n",
    "print('y val oh shape', y_val_oh.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\n",
    "nb_train_samples = new_train_x.shape[0]\n",
    "nb_val_samples = new_X_val.shape[0]\n",
    "print('nb val samples',nb_val_samples)\n",
    "nb_test_samples = X_test.shape[0]\n",
    "\n",
    "# dimensions of our images.\n",
    "img_width, img_height = 32, 32\n",
    "batch_size=100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is the augmentation configuration we will use for training\n",
    "aug_datagen = ImageDataGenerator(\n",
    "       rescale=1./255,         # sempre faz o rescale\n",
    "       shear_range=0.2,        # sorteio entre 0 e 0.2 distribuição uniforme\n",
    "       zoom_range=0.2,         # sorteio entre 0 e 0.2\n",
    "       horizontal_flip=True)   # sorteio 50%\n",
    "\n",
    "non_aug_datagen = ImageDataGenerator( rescale=1./255)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_generator = aug_datagen.flow(\n",
    "        x = new_X_train, y = y_train_oh,                       # as amostras de treinamento\n",
    "        batch_size=batch_size,shuffle=False                # batch size do SGD\n",
    "        )\n",
    "\n",
    "validation_generator = non_aug_datagen.flow(\n",
    "        x = new_X_val, y = y_val_oh,                  # as amostras de validação\n",
    "        batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_generator = non_aug_datagen.flow(\n",
    "        x = X_test, y = y_test_oh,                  # as amostras de validação\n",
    "        batch_size=batch_size, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T01:21:21.747496Z",
     "start_time": "2017-09-09T01:21:21.729673Z"
    },
    "collapsed": true
   },
   "source": [
    "#Conjunto de treinaemnto\n",
    "samples_train = train_datagen.flow(new_X_train)\n",
    "n_samples_train = nb_train_samples/batch_size\n",
    "\n",
    "#Conjunto de teste\n",
    "samples_test = train_datagen.flow(X_test)\n",
    "n_samples_test = nb_test_samples/batch_size\n",
    "#Conjunto de validacao\n",
    "\n",
    "samples_val = train_datagen.flow(new_X_val)\n",
    "n_samples_val = nb_val_samples/batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T01:21:25.667573Z",
     "start_time": "2017-09-09T01:21:25.660405Z"
    }
   },
   "outputs": [],
   "source": [
    "n_classes = len(np.unique(y_test))\n",
    "print(n_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinamento"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer_Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Subindo a VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(y_train_oh.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-09-09T00:58:01.450645Z",
     "start_time": "2017-09-09T00:57:23.054835Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "#modelvgg = VGG16(include_top=False, weights='imagenet',classes=y_train_oh.shape[1])\n",
    "modelvgg = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "\n",
    "modelvgg.layers = modelvgg.layers[:-4]\n",
    "modelvgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_generator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-f041aba97974>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_generator\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'train_generator' is not defined"
     ]
    }
   ],
   "source": [
    "train_generator.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1700, 512, 1, 1) float32\n",
      "(300, 512, 1, 1) float32\n",
      "(500, 512, 1, 1) float32\n"
     ]
    }
   ],
   "source": [
    "train_feature = modelvgg.predict_generator(generator=train_generator, steps=int(np.round(train_generator.n / batch_size)))\n",
    "print(train_feature.shape, train_feature.dtype)\n",
    "validation_features = modelvgg.predict_generator(generator = validation_generator, steps=int(np.round(validation_generator.n / batch_size)))\n",
    "print(validation_features.shape, validation_features.dtype)\n",
    "test_features = modelvgg.predict_generator(generator = test_generator, steps=int(np.round(test_generator.n / batch_size)))\n",
    "print(test_features.shape, test_features.dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1700, 512, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_feature.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 771       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 132,099\n",
      "Trainable params: 132,099\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def model_build():\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(train_feature.shape[1:])))\n",
    "    model.add(Dense(256, activation='relu'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "n_classes=3\n",
    "model = model_build()\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCb(TrainingPlotter):\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "\n",
    "def train_network(model, model_name, Xtra, ytra, Xval, yval, \n",
    "                  opt='rmsprop', batch_size=100, nepochs=50, patience=50, reset=False, ploss=1.0):\n",
    "\n",
    "    do_plot = (ploss > 0.0)\n",
    "    \n",
    "    model_fn = model_name + '.model'\n",
    "    if reset and os.path.isfile(model_fn):\n",
    "        os.unlink(model_name + '.model')\n",
    "        \n",
    "    if not os.path.isfile(model_fn):\n",
    "        # initialize the optimizer and model\n",
    "        print(\"[INFO] compiling model...\")\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])    \n",
    "\n",
    "        # History, checkpoint, earlystop, plot losses:\n",
    "        cb = MyCb(n=1, filepath=model_name, patience=patience, plot_losses=do_plot)\n",
    "        \n",
    "    else:\n",
    "        print(\"[INFO] loading model...\")\n",
    "        model, cb = load_model_and_history(model_name)\n",
    "        cb.patience = patience\n",
    "\n",
    "    past_epochs = cb.get_nepochs()\n",
    "    tr_epochs = nepochs - past_epochs\n",
    "    \n",
    "    if do_plot:\n",
    "        vv = 0\n",
    "        fig = plot.figure(figsize=(15,6))\n",
    "        plot.ylim(0.0, ploss)\n",
    "        plot.xlim(0, nepochs)\n",
    "        plot.grid(True)\n",
    "    else:\n",
    "        vv = 2\n",
    "\n",
    "    print(\"[INFO] training for {} epochs ...\".format(tr_epochs))\n",
    "    try:\n",
    "         model.fit(Xtra, ytra, batch_size=batch_size, epochs=tr_epochs, verbose=vv, \n",
    "                      validation_data=(Xval,yval), callbacks=[cb])\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    model, histo = load_model_and_history(model_name)\n",
    "    return model, cb\n",
    "\n",
    "\n",
    "def test_network(model_name, Xtest, ytest, batch_size=40):\n",
    "    model, histo = load_model_and_history(model_name)\n",
    "    print('Model from epoch {}'.format(histo.best_epoch))\n",
    "    print(\"[INFO] evaluating in the test data set ...\")\n",
    "    loss, accuracy = model.evaluate(Xtest, ytest, batch_size=batch_size, verbose=1)\n",
    "    print(\"\\n[INFO] accuracy on the test data set: {:.2f}% [{:.5f}]\".format(accuracy * 100, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAA4EAAAGDCAYAAACV/RXuAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAIABJREFUeJzs3Xl8VNX9//HXSQhrWMK+SoKyhCTIDopoEKVIK4obtm5o\nlRa1dvX71f7aqq1Wba1SvxWpe62oRaxL674QEQVkEZFVBMK+JEAgEMh6fn+cO8mQZJYsk2TI+/l4\n3MfMvefcc8/M3OQxnzn3no+x1iIiIiIiIiKNQ0x9d0BERERERETqjoJAERERERGRRkRBoIiIiIiI\nSCOiIFBERERERKQRURAoIiIiIiLSiCgIFBERERERaUQUBIpIo2WMyTDGWGPMtHo4tvWWxLo+ttQv\nY8zd3mf/XH33RUREGicFgSJS5/yCL2uMKTHGZBlj3jfGDKul9qd5bWfURnsR8ldvOVydnY0xlxtj\n1hhj8o0xmcaY/wlRP9PvPfdfMrzyjsaYBcaY/caYAmPMTmPMc8aY9l55YoD9rTHmbq9OeoDyy8r1\nZboxZrXX9wPGmPer8x40JsaY84wxC40xxyo7t40xycaYJcaYHO993WqM+asxpnk1jtXH77PbZYyJ\nrbUXcpIzxpxtjPnEGHPUGHPEGLPMGNPPr7yDMeYJY8wev8/plhBtBtzH739dZUu6V+d/jDH7vOVX\n5drdb4y5KEJvh4g0YE3quwMi0qgtAFYCY4DzgdOAPvXaozpirf1Zdfc1xpwB/As4CrwMjAceNMYc\nstb+PcBuzwDt/dYvAXoB33rrLYF44E2gBJgCXAcY7/EwLmgt7QZwCxDr14bPEmCx3/pGv77fAdzv\ntfcvoBg4I9RrFvrhPqPVwPBKyjsARcArQHPgUuA2IAe4q4rHutrveTfc+VUvgboxJs5aW1gfx64q\nY8xY4EPc38R/gT3A6UBH4BtjTDPgI2/b18AbQBfc/71AbYbaZy0n/l12By7H/Q1vMcakAQ8CC3F/\ns38yxrxrrV2N+ztcbK19o8YvXkSij7VWixYtWup0ATIAC/zMW0/z1i3Q1NvWEngAF2AcBVYAF/u1\ncT6w3Cs75JVfAkzza8u3ZIboxzRvPQ64E1jvtbsO+DkQ45UnAu8CB4FjwAbgHq8sAfcFPBs4DmwB\n/h7kPfD1LdFb/xmwCcj32sgA+gfY93Vv31966+ODvc5K9u/k9d8CgwLU+YVXviBA+YVe+W6/zyzd\n23Z3gH3aeO9rfqDXVsk+oT4T3+e9EHgEF/TsBK4K0mYT4H+8to7ivkhP9yu/22vzFeBFvzrjy72H\nTwHbcAHtYmBiuWP8FBe05QF7gd+Va/9fwNPAEdx5fl4Y78fPvH0zQtR71Kv3fDX+Pjd4+66orA2g\nJ/APYKt3rq8DRnhl7b1jb/LKNgPf88oyvfbSy312GeXOn0zvPdoPPIsLRBfg/i4KgSzgBaCdX5+S\ngdeAXbhzeyXQG3jCa/PXfnUfL7+t3Ou7FZgZYLk1yPv2KX7/Tyop/6FX/jHe+RvGZ1GlfYC/ePX/\n7a1f4a0PA0Z4z6/wnucBp1X1/NCiRcvJsdR7B7Ro0dL4FsqCr09wv2Iv9db/41fnJW/bcuBJ3Jfo\nEr8vkDtwIx/P40a5lgG/A0biRi2sV2cm3pfvIP2Y5q3/yVvfhftynuWt3+mVv+Ctvw/8Hfer/7te\n2b1e2SLvS+bbwNdB3oPSIBD3q771jjcbF3hs9r3WSvbd6tU/x1tv69deuzDe/996dT+qpGym934e\nwH2ZnhKgjY+8Nn7rty3d23YYF+htBn5PWZA4wSvfifvCfBRYA1wWpK+hPpNpfq/9C7/P/gjQJkCb\n93t11nvn1mZv/Tqv/G5vvQR4z1us198uuFspFlEWKP0TF5wUA2d6bdznled55S8Dcypp/0NcAGmB\nbWF8dgGDQFwANtM7Xh7ux4ozqvi3OdLv/TvLe54LtPTKWwLfeNs34IKsT4GLvPdlAWV/e095791t\n3r6ZhBcE+vZ/AvdjRD/c/4hngVl+x5/t7dcVFyD6Po8nvMfBwChv+xqvrvHatkBSiP8LlS0V3ndv\nnxa4/0cW9yPNIWA7cA9lP1i86JUvwP0NHwL+g/dDUIB2w94HN5Kf49Uf620b5K0vxo3Ql3jbvgD+\nUBv/z7Vo0RKdS713QIsWLY1vCfAlq4SyL/advG3FwP/hvtj69nnZq7MX90X1MqC/9wU01is74ctl\nGP2Y5n05PMKJwdVF3voub/1f3vqdwBDcZXe+Yz7olc3EfZGO95UFOLZ/EJjsPf8KFyj19OpUuj9u\nhMUCw7z1Jn7tDQjxmpviAioLXBikX74vjgMrqeMbuT0GdPLbfg7ukrVnvS+vvtHGP3rlP/Breykw\nx/uMi3yvpdxxwvlMfJ/1fu/ziKPsy/jwAG3meuXPeJ/Xm77X69W521tf4bffl962WygLlHKBVl75\nI962F8sdY4pfG03Ktb/aq5vk9750DPH5BQsCE8t9fu8APar4t+kbQZznrft+cLjKW7/M9/7jBYZ+\n5+Bwv/OiWyWvO5PwgsASyo1Q4f7e/gd4CDfiZ4FvvLJf+T4v/EbL/I67yisf6tfHz2v5f1p3v/d9\nO+7HhYPeuu+KB98PFMW4c9/349eXQdoNex/gJ17Z8nLb/xfY5y2/AqZ7n0UfYB5u1HYe0LU23xMt\nWrQ07EUTw4hIffq5tdbggrgDwB+NMWfjvsyCC+xuxV1Wd463zXcvzI9wlyK+ghvR2Ye7F6a6OgGt\nvOfrvMf13mM3Y0xT3Jf3z4E/4L5w5uAuWQUXTLwH3Iz7xT0HeN4YE/L/rLV2He6+rR5eG9uNMetx\nwWFl9nqP8eUewd2HFMxU3OV13+LuWyrfFwO0w41kjcLdg1Se737GF621WX7bF1hr06y111trf+BX\n71Lv0b/uJGvtVbjLa2OB71ZynHA+E5911trj1t0/dtTb5v+++HT023497ty60Fsvf2/W+kqe96Ts\n/NxurT1arrx3uWOU3htprS0q1/5Ka63FnSs+lfU5LNbaTO/z64wLxCcCz4W7vzGmCe78ADea5f94\njfeY5D1+ba3N8zt2kV/ZNmvt7nJllQk04cxea23pfabGmO/j/t4eBH4JXOwVdSrXp6XW2pJKjvu0\n93g1MNl7PifAsTHG3GqMmRlguTXAbr6RSIBfWGtvAv7orfuO6Tv/3/XO/Une+mBjzCkB2g1rH2OM\nwd0DCu5/USlr7YPW2s7W2s64Hz7+iPvbvBf3v/e7uP81fwnQBxE5CSkIFJF6Z639BhfQgbv0K9N7\nXoAbaTLel9umuAlLAN6x1vbFfeG+DDcxxn1eWbH3WJX/cVm4S+gABniP/b3H3dbaAmCztXYM7vLL\nkbjA9VfGmF7AAWvtRKA1bhKHNbiRrzGhDuzNvniftbYjLoh40Dv2zwPsstJ7HOk9jvAet1lrc7w2\nB3hLs3L7/tR7/KsXgPj60Nr33Fp7CHjLW00yxsT51evkvS4o92UT6ON9GS3P98V8LWWfTXlHKtkW\nzmfi4x9oWALLpixIPN3v3Iqh4oQrAyp5voOy87OXMaZluX5t9Y7hez2jfA14QZY/X5+D9Tcs5T6/\nLOADb9V/ZspTvHMiIUAzE3ABJMA/jTGWssDiPGNMV9y9rgBpxpgWfm038Ss7xavrXwZl73sb7zE1\nQD/yy637AtO/A8381n3nmu+4I/x/dPE77gtem9/H/f8oAuYGODa4/yc/DbBcVtkO3rm4IUB7vnNh\nVZBjHvH6fKr3GcWHu4/ne7gfMXbjrlgI5H5csPw67nLZddba9bi/zcFB9hORk4xmBxWR+jTFuDx5\n/XBfCEuAL6y1WcaYubgJDJYYYz7ABXljcffM3Q18aYzJxE3M0ctrzzeist17HGaMmYW7dOrJYB2x\n1lqv7q+AF40x71L2C/7fvMdZxpj+uC97sbgAtBj3ZewOY8xk3OWQBZSNFh0K433o5b3OBbgRTV/g\nmBOg/p9wo1d3GWNSgfO87Q/41fGNnA3BCxq92QuHee0+V67NXxpjLsXdg1mM+1IJ8LE9cXbGH+Mu\nu/zIWvt1uTZ+C4wxxnyOuyzTF7DPAbDW7jTGzAGuBd42xnyDG606Avy7/IsM8zOpEq/Nx3CXFr5v\njPkPbvRtNO4e1Wl+1U83xrznPR+MC0hfxX1GS3AB3qfGmDW4AMMCs7xjPAr8GphjjHnVez+KOXHm\nzbAZY84CbgQGepsGGJdncL219gHgUWPMQNz514yy9+k9v2aex42o/5yKATx+fdvsteNzNm7io+/j\nArGNQF/c3+AnuAD5Ydy9ap/i/k6Xep9XT9xlqY/iLqkdCNxrjBmPO5fC4Rv5noR7HyeVK38BuAN3\nrn9hjFmOO89vxI227jfGvI4LHrviRtWyCMBamx5mv8r7E26k7WFjzATKRsCf9x6f8Po50fs78AXo\n/7XWHvCef4T7IWgKbhQ2nH2gbNR9VrkfR0oZY4bjRnQHeZvWA98zxjyL+3t/p+ovWUSiVn1fj6pF\ni5bGt1DxnsDDuIldLverE4/71Xoj7h64nbh7gUZ75b4ZCI/hAq35wBCvLAYXeBz22v9viH5M89ab\nAr/BTTyRh/uS9CvK7vv7IS6gyvXKvwamemUXUnYZ6HGvjZ8EeQ/87wlsj/sCvQsXQO7DTYyTEGT/\nqbhf7wtwgfAdgKmk/cF+2+Z52x6qpL2LcZfcHfbe0024QCHBr04cZfcTfi9AGwtw9+flef37BSfe\np9UKF0hke8f6xPeZBnidoT6TaZS7R46yyTHSA7QZhwsC13qvdS/ustQLvPK7vf1fxs2C6ZuVdIJf\nG75L67Z758MX/u8J7kfWnxF8dtDnvPV2/udDgD5P48S/Gd+S4ZVPx40+H8UF1etwk/I0q+R8/1kl\n7cd7+1rcpbr+ZXfhd68Z7keL53HnXaDZQTdTNjnQhV5ZIu5y6lxcsOibydL3GtK99cxyx++Omx3T\nN+un7963HL86vtlBd/vV6+1Xfp7fe3ZNBP+3/QI3UnzM++yvK1c+wnvtx3D/02Zz4iynmV4fL67C\nPqmU3YtZ6T2luFHTJcC9ftv64GbVPQp8BvSJ1PuiRYuWhrcYa2t8FYqIiMhJwxhzNy7w+Ye1dlr9\n9kZqg3eZ6GFcMNTFWlvZ5cciIo2GLgcVERGRk5Yx5jLcZcetcLk7FQCKSKOnIFBEREROZrcCZ+Lu\nt7uznvsiItIg6HJQERERERGRRkQpIkRERERERBoRBYEiIiIiIiKNSNTdE5iQkGBPPfXUgOUlxS4P\ncUxsbETKw6lTXOjy/8bGVf/tDXWMonyXBqhJs6bVPkZdiJZ+1pajR4/SqlWr+u6GSAU6N6Wh0rkp\nDZnOT2moli9fnm2t7VTd/aMuCOzVowfLli0LWJ6zax8A7bp3jkh5OHWyt+wAoGNSz4BthBLyGJtd\nLuyOfXpVWt5QREs/a0tGRgbp6en13Q2RCnRuSkOlc1MaMp2f0lAZY7bWZH9dDioiIiIiItKIKAgU\nERERERFpRBQEioiIiIiINCJRd0+giIiIiIhUT2FhITt27OD48eP13RUJQ/PmzenZsydxcXG12m7U\nBYH5hYVBy4NN6FIb5eHUqcmEMGEfI0omWomWfoqIiIg0Bjt27KB169YkJiZijKnv7kgQ1lr279/P\njh07SEpKqtW2dTmoiIiIiEgjcfz4cTp06KAAMAoYY+jQoUNERm2jLghsEiR/H0BezmHycg5HrDyc\nOod2Z3Fod1bQNkIJdYzsLTtKU1E0ZNmbt5emiRARERGR+qcAMHpE6rOKuiAwNiZ4lwvyjlOQFzha\nrml5OHUKj+dTeDw/aBuhhOyHtW4REREREYkSOTk5zJo1q1r7Tpo0iZycnKB1fve73/Hhhx9Wq/3G\nJOqCQBERERERiU7BgsCioqKg+7799tu0a9cuaJ3f//73nHfeedXuX2OhIFBEREREROrEHXfcwaZN\nmxg8eDC33347GRkZjB07lsmTJzNw4EAALr74YoYNG0ZKSgpPPPFE6b6JiYlkZ2eTmZlJcnIyN910\nEykpKUyYMIFjx44BMG3aNObNm1da/6677mLo0KGkpaWxfv16ALKysjj//PNJSUnhxhtvpHfv3mRn\nZ1foa3x8PLfffjspKSmcd955fPHFF6Snp9OnTx/efPNNANasWcPIkSMZPHgwgwYNYuPGjQC88MIL\npdt/9KMfUVxcHLk3tRqibnZQERERERGpucQ73opIu5kPfDdg2QMPPMDq1atZuXIlABkZGaxYsYLV\nq1eXzoD5zDPP0L59e44dO8aIESO49NJL6dChwwntbNy4kZdeeoknn3ySK664gldffZWrr766wvE6\nduzIihUrmDVrFg899BBPPfUU99xzD+eeey533nkn7777Lk8//XSlfT169Cjnnnsuf/7zn5kyZQq/\n+c1v+OCDD1i7di3XXXcdkydPZvbs2fz0pz/lqquuoqCggOLiYtatW8e//vUvPvvsM+Li4rj55puZ\nM2cO1157bXXf0lqnIFBEREREROrNyJEjT0iB8Oijj/Laa68BsH37djZu3FghCExKSmLw4MEADBs2\njMzMzErbvuSSS0rr/Pvf/wZg4cKFpe1PnDiRhISESvdt2rQpEydOBCAtLY1mzZoRFxdHWlpa6fHO\nOOMM7rvvPnbs2MEll1xC3759+eijj1i+fDkjRowA4NixY3TuHDoNXV2KuiBQeQK9Y0RJ/r1o6aeI\niIhIYxNsxK4utWrVqvR5RkYGH374IYsWLaJly5akp6dXmiKhWbNmpc9jY2NLLwcNVC82NjbkPYfl\nxcXFlc7OGRMTU9pWTExMaVs/+MEPGDVqFG+99RaTJk3i73//O9ZarrvuOu6///4qHa8u6Z5AERER\nERGpE61btyY3Nzdg+aFDh0hISKBly5asX7+exYsX13ofxowZw9y5cwF4//33OXjwYLXb2rx5M336\n9OG2227joosuYtWqVYwfP5558+axb98+AA4cOMDWrVtrpe+1JeqCQOUJdJQnUERERESiTYcOHRgz\nZgypqancfvvtFconTpxIUVERycnJ3HHHHYwePbrW+3DXXXfx/vvvk5qayiuvvELXrl1p3bp1tdqa\nO3cuqampDB48mNWrV3PttdcycOBA7r33XiZMmMCgQYM4//zz2b17dy2/ipoxNspyzQ1KTbWrVq8O\nWJ6zy0XcgS6nrGl5OHV8wVlNLgsNeQwvsGrol1tGSz9rS0ZGBunp6fXdDZEKdG5KQ6VzUxqyk/H8\nXLduHcnJyfXdjXqVn59PbGwsTZo0YdGiRcyYMaN0opqGqLLPzBiz3Fo7vLptRuyeQGPMM8D3gH3W\n2tQg9UYAi4ArrbXzItUfERERERGRbdu2ccUVV1BSUkLTpk158skn67tLdS6SE8M8B/wNeD5QBWNM\nLPAg8H4E+yEiIiIiIgJA3759+fLLL+u7G/UqYvcEWmsXAAdCVPsJ8CqwL1L9EBERERERkTIRvSfQ\nGJMI/Leyy0GNMT2AF4FxwDNevUovBzXGTAemAyT36z9s1t9nBzxmUufuAGzZtysi5eHUSe6ZCMC6\nHZkB2wgl1DFSevUBYM32zdU+Rl2Iln7WliNHjhAfH1/f3RCpQOemNFQ6N6UhOxnPz7Zt23LaaafV\ndzekCr799lsOHTp0wrZx48bV6J7A+gwCXwH+Yq1dbIx5jiBBoL/+/fvbDRs21HZXRWrFyXgDuZwc\ndG5KQ6VzUxqyk/H81MQw0SeqJoYJw3DgZS8BY0dgkjGmyFr7ej32SURERERE5KRWb3kCrbVJ1tpE\na20iMA+4OZwAUHkCHeUJFBEREZHGwHdJ7q5du7jssssqrZOens6yZcuCtjNz5kzy8vJK1ydNmkRO\nTk7tdTSKRCwINMa8hEv90N8Ys8MY80NjzI+NMT+uSbuxMcG7XJB3nIK84xErD6dO4fF8Co/nB20j\nlJD9sNYtIiIiIiKNQPfu3Zk3r/oZ5coHgW+//Tbt2rWrja5FnUjODvp9a203a22ctbantfZpa+1s\na22FWV2stdOUI1BERERE5OR2xx138Nhjj5Wu33333Tz00EMcOXKE8ePHM3ToUNLS0njjjTcq7JuZ\nmUlqqptq5NixY1x55ZUkJyczZcoUjh07VlpvxowZDB8+nJSUFO666y4AHn30UXbt2sW4ceMYN24c\nAImJiWRnZwPw8MMPk5qaSmpqKjNnziw9XnJyMjfddBMpKSlMmDDhhOP4TJs2jRkzZjB69Gj69OlD\nRkYGN9xwA8nJyUybNg2A4uJipk2bRmpqKmlpaTzyyCMAbNq0iYkTJzJs2DDGjh3L+vXra/oWh6U+\n7wkUEREREZH6cnfbCLV7KGDR1KlT+dnPfsYtt9wCwNy5c3nvvfdo3rw5r732Gm3atCE7O5vRo0cz\nefJkvPlDKnj88cdp2bIl69atY9WqVQwdOrS07L777qN9+/YUFxczfvx4Vq1axW233cbDDz/M/Pnz\n6dix4wltLV++nGeffZYlS5ZgrWXUqFGcc845JCQksHHjRl566SWefPJJrrjiCl599VWuvvrqCv05\nePAgixYt4s0332Ty5Ml89tlnPPXUU4wYMYKVK1dSXFzMzp07Wb16NUDpZajTp09n9uzZ9O3blyVL\nlnDzzTfz8ccfV+39rgYFgSIiIiIiUieGDBnCvn372LVrF1lZWSQkJNCrVy8KCwv59a9/zYIFC4iJ\niWHnzp3s3buXrl27VtrOggULuO222wAYNGgQgwYNKi2bO3cuTzzxBEVFRezevZu1a9eeUF7ewoUL\nmTJlCq1atQLgkksu4dNPP2Xy5MkkJSUxePBgAIYNG0ZmZmalbVx44YUYY0hLS6NLly6kpaUBkJKS\nQmZmJueccw6bN2/mJz/5Cd/97neZMGECR44c4fPPP+fyyy8vbSc/v2a3lIVLQaCIiIiISGMUZMQu\nki6//HLmzZvHnj17mDp1KgBz5swhKyuL5cuXExcXR2JiIsePB5+nozJbtmzhoYceYunSpSQkJDBt\n2rRqtePTrFmz0uexsbGVXg7qXy8mJuaEfWJiYigqKiIhIYGvvvqK9957j9mzZzN37lxmzpxJu3bt\nWLlyZbX7V131NjtodeUXFgYtb9e9M+26d45YeTh1Oib1pGNSz6BthBLyGH160bFPrxodoy5ESz9F\nREREpG5MnTqVl19+mXnz5pWOgh06dIjOnTsTFxfH/Pnz2bp1a9A2zj77bF588UUAVq9ezapVqwA4\nfPgwrVq1om3btuzdu5d33nmndJ/WrVuTm5tboa2xY8fy+uuvk5eXx9GjR3nttdcYO3Zsbb1cALKz\nsykpKeHSSy/l3nvvZcWKFbRp04akpCReeeUVAKy1fPXVV7V63EA0EigiIiIiInUmJSWF3NxcevTo\nQbdu3QC46qqruPDCC0lLS2P48OEMGDAgaBszZszg+uuvJzk5meTkZIYNGwbA6aefzpAhQxgwYAC9\nevVizJgxpftMnz6diRMn0r17d+bPn1+6fejQoUybNo2RI0cCcOONNzJkyJCAl35Wx86dO7n++usp\nKSkB4P777wfcCOiMGTO49957KSws5Morr+T000+vteMGYmyUpRlIGTjQrlm7NmC5L7dey3ZtIlIe\nTh1fjsC23ToFbCOUUMfw5Qis6YhjpPlyBDaW0cCMjAzS09PruxsiFejclIZK56Y0ZCfj+blu3TqS\nk5PruxtSBZV9ZsaY5dba4dVtM+ouB1WeQI/yBIqIiIiISDVEXRAoIiIiIiIi1acgUEREREREpBFR\nECgiIiIiItKIKAgUERERERFpRKIuRUQ4eQIjWR5OndqYsTPkMaJkts1o6aeIiIiISGOhkUARERER\nEWmw4uPjAdi1axeXXXZZpXXS09NZtmxZ0HZmzpxJXl5e6fqkSZPIycmpvY6GUNfHCybqgsAmsbFB\ny/NyDpfm2ItEeTh1Du3OKs0VWF2hjpG9ZUdprsCGLHvz9tJcgSIiIiIi1dW9e3fmzZtX7f3LB4Fv\nv/027dq1q42uhaWujxdM1AWByhPoUZ5AEREREYkyd9xxB4899ljp+t13381DDz3EkSNHGD9+PEOH\nDiUtLY033nijwr6ZmZmkpqYCcOzYMa688kqSk5OZMmUKx44dK603Y8YMhg8fTkpKCnfddRcAjz76\nKLt27WLcuHGMGzcOgMTERLKzswF4+OGHSU1NJTU1lZkzZ5YeLzk5mZtuuomUlBQmTJhwwnF8pk2b\nxowZMxg9ejR9+vQhIyODG264geTkZKZNm1Zaz3e8cNuNpKgLAkVEREREpHbk7NpXYfG/Gq2q5aFM\nnTqVuXPnlq7PnTuXqVOn0rx5c1577TVWrFjB/Pnz+eUvf4kNMuDx+OOP07JlS9atW8c999zD8uXL\nS8vuu+8+li1bxqpVq/jkk09YtWoVt912G927d2f+/PnMnz//hLaWL1/Os88+y5IlS1i8eDFPPvkk\nX375JQAbN27klltuYc2aNbRr145XX3210v4cPHiQRYsW8cgjjzB58mR+/vOfs2bNGr7++mtWrlxZ\noX647UaKgkAREREREakTQ4YMYd++fezatYuvvvqKhIQEevXqhbWWX//61wwaNIjzzjuPnTt3snfv\n3oDtLFiwgKuvvhqAQYMGMWjQoNKyuXPnMnToUIYMGcKaNWtYu3Zt0D4tXLiQKVOm0KpVK+Lj47nk\nkkv49NNPAUhKSmLw4MEADBs2jMzMzErbuPDCCzHGkJaWRpcuXUhLSyMmJoaUlJRK9wm33UiJutlB\nRURERESkdtTFzPrlXX755cybN489e/YwdepUAObMmUNWVhbLly8nLi6OxMREjh8PfotWZbZs2cJD\nDz3E0qVLSUhIYNq0adVqx6dZs2alz2NjYwNetumrFxMTc8I+MTExFBUVVbvdSNFIoIiIiIiI1Jmp\nU6fy8ssvM2/ePC6//HIADh06ROfOnYmLi2P+/Pls3bo1aBtnn302L774IgCrV69m1apVABw+fJhW\nrVrRtm1b9u7dyzvvvFO6T+vWrcnNza3Q1tixY3n99dfJy8vj6NGjvPbaa4wdO7a2Xm6DFHUjgcoT\n6B0jSvJK+Z24AAAgAElEQVTvRUs/RURERKRupKSkkJubS48ePejWrRsAV111FRdeeCFpaWkMHz6c\nAQMGBG1jxowZXH/99SQnJ5OcnMywYcMAOP300xkyZAgDBgygV69ejBkzpnSf6dOnM3HixNJ7A32G\nDh3KtGnTGDlyJAA33ngjQ4YMqfNLNOuSCXbDZUPUv39/u2HDhvruhkilMjIySE9Pr+9uiFSgc1Ma\nKp2b0pCdjOfnunXrSE5Oru9uSBVU9pkZY5Zba4dXt82ouxxUeQId5QkUEREREZHqiLogUHkCPcoT\nKCIiIiIi1RB1QaCIiIiIiIhUn4JAEREREZFGJNrmBGnMIvVZKQgUEREREWkkmjdvzv79+xUIRgFr\nLfv376d58+a13nbUpYgQEREREZHq6dmzJzt27CArq2aTGErdaN68OT171jz9XHlRFwQqT6B3jCjJ\nvxct/RQRERFpDOLi4khKSqrvbkg90+WgIiIiIiIijUjUBYHKE+goT6CIiIiIiFRH1AWByhPoUZ5A\nERERERGphqgLAkVERERERKT6FASKiIiIiIg0IgoCRUREREREGpGIBYHGmGeMMfuMMasDlF9ljFll\njPnaGPO5Meb0SPVFREREREREnEjmCXwO+BvwfIDyLcA51tqDxpgLgCeAUaEaVZ5A7xhRkn8vWvop\nIiIiItJYRCwItNYuMMYkBin/3G91MVDzyElERERERESCaij3BP4QeCecisoT6ChPoIiIiIiIVIex\nEcw1540E/tdamxqkzjhgFnCWtXZ/gDrTgekAyf36D5v199kBj5nUuTsAW/btikh5OHWSeyYCsG5H\nZsA2Qgl1jJRefQBYs31ztY9RF6Kln7XlyJEjxMfH13c3RCrQuSkNlc5Nach0fkpDNW7cuOXW2uHV\n3b9eg0BjzCDgNeACa+034bQ5KDXVrlpd6VwzAOTs2gcEvqeupuXh1PGN0NXk3sCQx/BG1xr6PXfR\n0s/akpGRQXp6en13Q6QCnZvSUOnclIZM56c0VMaYGgWB9XY5qDHmFODfwDXhBoAiIiIiIiJSMxGb\nGMYY8xKQDnQ0xuwA7gLiAKy1s4HfAR2AWcYYgKKaRLMiIiIiIiISWiRnB/1+iPIbgRsjdXwRERER\nERGpKJJ5AiNCeQK9Y0TJPXbR0k8RERERkcaioaSIEBERERERkToQdUGg8gQ6yhMoIiIiIiLVEXVB\nYGxM8C4X5B2nIO94xMrDqVN4PJ/C4/lB2wglZD+sdYuIiIiIiEgVRF0QKCIiIiIiItWnIFBERERE\nRKQRURAoIiIiIiLSiCgIFBERERERaUSUJ7CK5eHUUZ7AMtHSTxERERGRxkIjgSIiIiIiIo1I1AWB\nyhPoKE+giIiIiIhUR9QFgcoT6FGeQBERERERqYaoCwJFRERERESk+hQEioiIiIiINCIKAkVERERE\nRBoRBYEiIiIiIiKNiPIEVrE8nDrKE1gmWvopIiIiItJYaCRQRERERESkEYm6IFB5Ah3lCRQRERER\nkeqIuiBQeQI9yhMoIiIiIiLVEHVBoIiIiIiIiFSfgkAREREREZFGJOqCwMP5JRQWl9R3N0RERERE\nRKJS1AWBu48UsinrSH13Q0REREREJCpFXZ5AgHW7DzOga5tKy5QnsGGJln6KiIiIiDQWUTcSCLB2\nV/AUDiIiIiIiIlK5oCOBxpihYbRRaK39upb6E5Z1u3MDlvly67VsV/lIYU3Lw6njyxHYtlungG2E\nEuoYvhyBtTHqGEm+HIEaERQRERERaRhCXQ76CbAUMEHqJAGJtdWhcKzbfRhrLcZU7JYvt16g4Kmm\n5eHUqWmOwLD6oRyBIiIiIiJSDaGCwKXW2nODVTDGfFyL/QkpxsD+owXsy82nS5vmdXloERERERGR\nqBf0nsBQAWC4dWpTs1jX5bW7dV+giIiIiIhIVYU1MYwxZrgxZooxZrIxZkCkOxVM8ybuElBNDiMi\nIiIiIlJ1oSaGOQf4C5ADDAM+AxKMMYXANdba7ZHv4omaxbogcJ1GAkVERERERKos1D2BM4EJ1tos\nY0wS8LC1dowx5nzgaWBCxHtYTgwlWAJfDqo8gQ1LtPRTRERERKSxCHU5aKy1Nst7vg3oDWCt/QDo\nEcmOBRIXC01iDFuyj5JXUFQfXRAREREREYlaoYLAZcaYp40xVwEvAhkAxpiWQGyE+1apuNhYTu0U\nj7WwYU/FfIF5OYdLc+xVpqbl4dQ5tDurNFdgdYU6RvaWHaW5Ahuy7M3bS3MFioiIiIhI/QsVBP4I\nWA6cAXwI3O5tt8B3gu1ojHnGGLPPGLM6QLkxxjxqjPnWGLMqzMT0xMbEMLC7y51XWdL4grzjpTn2\nKlPT8nDqFB7Pr3GuwJD9sFa5AkVEREREpMpCpYgotNbOstbeaq190lpb7G0/Zq3dGqLt54CJQcov\nAPp6y3Tg8XA7ndytNaDJYURERERERKoqaBBojJno97ytd2noKmPMi8aYLsH2tdYuAA4EqXIR8Lx1\nFgPtjDHdwun0wG5tAeUKFBERERERqSpjg1xSaIxZYa0d6j1/CtgDPAlcApxjrb04aOPGJAL/tdam\nVlL2X+ABa+1Cb/0j4H+ttcsqqTsdN1pIcr/+wx74v8e57eM8msfCrPNaEmNMad2kzt0B2LJvV6V9\nqml5OHWSeyYCsG5HZsA2Qgl1jJRefQBYs31ztY9RF6Kln7XlyJEjxMfH13c3RCrQuSkNlc5Nach0\nfkpDNW7cuOXW2uHV3b8qQeBKa+1gv7IT1gPsn0gtBIH+BqWm2lWrVzPqjx+y93A+Gb9KJ7Fjq9Ly\nnF37gMApFmpaHk4d34QtNUkVEfIY3mQrDT0FQ7T0s7ZkZGSQnp5e390QqUDnpjRUOjelIdP5KQ2V\nMaZGQWCoPIGdjTG/AAzQ1hhjbFnUGGpSmVB2Av6RQU9vW1D5hYUAJHdrw97DWazbffiEIFB5AhuW\naOmniIiIiEhjESqQexJoDcTjJnrpCGCM6QqsrOGx3wSu9WYJHQ0cstbuDnfngd3cDKG6L1BERERE\nRCR8QUcCrbX3BNi+B7g22L7GmJeAdKCjMWYHcBcQ5+0/G3gbmAR8C+QB14fV4ViXnjC5my9NxIlB\noC+3Xst2bSrdv6bl4dTx5Qhs261TwDZCCXWM2rjktC40tstBRUREREQaulCXg57AGHMWMBJYba19\nP1hda+33Q5Rb4JaqHB9cnkAoCwLX7joxCPTl1gsUPNW0PJw6Nc0RGFY/lCNQRERERESqIVSKiC/8\nnt8E/A13eehdxpg7Ity3oJI6tqJ5XAy7Dh0nJ6+gPrsiIiIiIiISNULdExjn93w6cL53iegE4KqI\n9SoMsTGG/l19l4Tm1mdXREREREREokaoIDDGGJNgjOkAxFprswCstUeBooj3LgRNDiMiIiIiIlI1\noe4JbAssx6WIsMaYbtba3caYeG9bvRrYrTVQcXIYERERERERqVyo2UETAxSVAFNqvTdh8OUJBBjY\nveLkMMoT2LBESz9FRERERBqLKid8N8a0t9bmWWu3RKJDVeG7J/DbfUcoKCqp596IiIiIiIg0fKFm\nBx1jjFlnjFljjBlljPkAWGqM2W6MOaOO+ngCX55AgPhmTejdoSUFxSVsyjoCuPx6vhx7lalpeTh1\nDu3OKs0VWF2hjpG9ZUdprsCGLHvz9tJcgSIiIiIiUv9CjQQ+AlwB3Ai8BdxjrT0VuAh4KMJ9q5Qv\nT6DPwHJJ4wvyjpfm2KtMTcvDqVN4PL/GuQJD9sNa5QoUEREREZEqC5kiwlr7tbV2EZBlrV0IYK1d\nAbSIeO/CEChpvIiIiIiIiFQUMkWE3/M7y5U1reW+VEvpSOAeBYEiIiIiIiKhhAoCf2uMaQlgrX3d\nt9EYcyrwfCQ7Fq7k7mUJ460ujxQREREREQkqVIqINwNs3wT8KSI9qqLubZvTtkUcB44WsPdwPs3r\nu0MiIiIiIiINWNAg0BjTB/gNsAt4ADdRzBnAOuB2a21mpDtYnn+eQABjDMndWrN48wHW7T7MuAHK\nE9iQREs/RUREREQai1CXgz4HLAWOAIuB9cAFwLvAMxHtWRUM7NYWgLW7dV+giIiIiIhIMKGCwNbW\n2settQ8Abay1f7HWbrfWPg0k1EH/KvDPE+iT3K014IJA5QlsWJQnUERERESkYQkVBJYYY/oZY0YA\nLY0xwwGMMacBFaOxOlA+TyCUpYlYt+uw8gSKiIiIiIgEEfSeQOB/gP8AJcDFwJ3GmNOBNsBNEe5b\n2Pp2iadJjGHL/qMcKyymRVy9xKciIiIiIiINXqjZQT8C+vttWmiM6QgctNYWR7RnVdCsSSyndY5n\n/Z5cNmYfY1C3+PrukoiIiIiISIMU6nLQCqy12dbaYmNM10h0qLp8SeO/yc6r556IiIiIiIg0XFUO\nAv08XWu9qAW++wI3KAgUEREREREJKNQ9gQFZa79bmx0JV/k8gT4Du7sgcNOhwqA59pQnsG5FSz9F\nRERERBqLsIJAY0wXoIe3utNauzdyXaoe30jg+j25lJRYYmJMPfdIRERERESk4QkaBBpjBgOzgbbA\nTm9zT2NMDnCztXZFhPtXQWV5AgHat2pKlzbN2Hs4n/WZ+xjYp0ul9Xy591q2a1Ot8nDq+HIEtu3W\nKWAboYQ6hi9HYG2MOkaSL0egRgRFRERERBqGUPcEPgf81FqbbK09z1sGAD8Dno147ypRWZ5AH9/k\nMGu2HwxYR3kCRURERESkMQsVBLay1i4pv9FauxhoFZkuVZ/vktAvdx+p556IiIiIiIg0TKGCwHeM\nMW8ZY6YaY870lqnGmLeAd+uig1Vx/kB3Cei8r/exdf/Reu6NiIiIiIhIwxM0CLTW3gb8DRgH3Okt\n44DHrLW3Rr57VTPklAS+N6ADBcWWu99cg9XlkiIiIiIiIicIOTuotfYd4J066Eut+PlZvcjYnMP8\nDVl8uG5f6eigiIiIiIiI1CBPYH0pyTsQtPzU03rwy+8UcM9/1nLPf9Ywtm9HmseVzSiqPIF1K1r6\nKVJteQegZfv67oWIiIhI2ELdE9jgND++F756OWida0b3ZkDX1uw4eIxZGZvqqGci0qjs3wT/uhoe\nGwX5ufXdGxEREZGwBQ0CjTHfN8Z0qKvOhO31m+Gb9yotyss5TEHuEf5wcSoAsz/ZRGb20RPKfTn4\nAu0frDycOod2Z5XmCqyuUMfI3rKjNFdgQ5a9eXtprkCRWlNUACtfcoFYXcs7AO/cAY+NhHX/gYIj\nsLPOU6aKiIiIVFuokcBTgFeMMZ8aY+42xowyxpi66Fggxc07gi2GudfC1s8rlPvy641IbM8lQ3tQ\nUFTCPf8pmyRGeQJFolz+EXjxCnj9xzDrDFjwEBQXRv64hcfhs0fhr4NhyeNQUgxDroafrIA+50T+\n+CIiIiK1JNTsoA9aa88FJgFfATcAK4wxLxpjrjXG1PmsK8UtOsHQ66DoOLx4Jez5OmDdOy9IpnWz\nJqWTxIhIlMs7AM9fBJvnQ9N4KM6Hj/8AT4yL3GictfD1PHhsBHzwW8g/BH3GwY8XwkWPQZtukTmu\niIiISISEdU+gtTbXWvuatfZH1tohwL1AJ+D5iPauMgb43iOQPNl9GfvnJXBgc6VVO7Vuxi8n9APg\n7jfXcKyguA47KiK16vAuePYC2LkM2p4CP1oA17wO7XrD3q/hqfHw3v+DgryaH6ukBHL3wrcfwlPn\nwas/hJxt0CkZrnoVrnkNuqbW/DgiIiIi9aBas4Naa9cCa4G/BKtnjJkI/BWIBZ6y1j5Qrrwt8ALu\nstMmwEPW2mdDdiAmFi59CuZcDls+gecvhh++D627Vqh69ejevLx0O+v35PJ4xrfckJoQ5qsUkQYj\n+1v45xQ4tA06DXBBWJvu0OFUuHkRzP8jLJ4Fi/4G6/8LF/4V+qQHbq+4CA5th4OZ7vHQDrfkbHOP\nh3dCcUFZ/fguMO7/weCrIDbqJlUWEREROUHEvs0YY2KBx4DzgR3AUmPMm14A6XMLsNZae6ExphOw\nwRgzx1pbUEmTJ2rSDK6cA/+YDLtWuBHB69+qWC02hj9cnMrlsxcxe8FmzuuZwintmtfKaxSRcooL\nYXOGu1+310g47fyaB027v3J/33nZ0GM4XPXKiSkZmraC79wHqZfAm7fB3tXuktEhV8OoGS6gO7DF\nXTHgW3K2QklR8OO27ABte0K/C+DMn0Cz+Jq9DhEREZEGIpI/aY8EvrXWbgYwxrwMXIQbQfSxQGtv\nspl44AAQ9JtZfqHfBBDNWsNV8+DZibBvDbx4Je2ueQ2atjxhnxGJ7bl0aE9eXbGDh5fs4dlpIwK2\nrzyBtSta+ik1UFIC2z53982tfQOO+eXyjO8Cg6bCkGugU7+qt535Gbx0JeQfdvfhTX0hcDDWYxhM\nz4DPZsInf4IvX3BLIK27Q/skaHeKC/ba9oS2vbylhwsuRURERE5CVQ4CjTGTrbVvhlG1B+CfG2AH\nMKpcnb8BbwK7gNbAVGttSZU61KqDuzTs6e/A9sXwynVw5YsQG3dCtTsuGMD7a/eQsSGLD9buZUJK\nxUtHRSRM1roR+K9fhTX/htzdZWUd+7mAbdPHsH8jfP6oW3qOhKHXQMoU9wNOKOvfhnnXu0mgUqbA\nlL+7KwCCiY2Ds2+H5IvgvTshe6ML9Nr3KVsSkiAhscKPRSIiIiKNhbFB0gwYYy4pvwl3iefNANba\nfwfZ9zJgorX2Rm/9GmCUtfbWcnXGAL8ATgU+AE631h4u19Z0YDpAUmLisGeerXjbYMujOxjy5R3E\nFeVyNCGFNafNIK/ViaNQH2wtZM66Ato3j+HOkc3o1LLivDgdWrcFYH/uoUAvLWSdUzq6AHNb9p6A\nbYQS6hjJPRIBWLczs9rHqAsDeyYBsHbHlnruSd04cuQI8fEn52WDMcUFtD20loSDX9IpazEtjped\n38ead2Zf57Hs63w2R1v1BmPAWtoc3kDXPR/Sed+nNCl2KU+KY5qR1WkMh9v0Jbb4ODElBcQWH/ee\n5xNbnE9s8XHaH/gSQwm7un2Hb/r9CExsfb30k8LJfG5KdNO5KQ2Zzk9pqMaNG7fcWju8uvuHCgIL\ngfeAfbgAEOAyYB5grbU3BNn3DOBua+13vPU7cTvd71fnLeABa+2n3vrHwB3W2i8CtTsoNdWuWr26\n8sKdKyiZM5WYvH0Q2xTS74AzbysdFSwqLmHqYwtZviuXXu1bMPdHZ9CtbYsTmsjZ5VJJBLscM1Qd\nXxL3mlwWGvIYXgL2hn65ZbT0s7ZkZGSQnp5e392oHda6kbRNH8G3H0HmQig6VlYe3wVSLoHUS6Hn\ncBf4BVJw1F0q+uULsPWz8Psw9ldw7m+Cty1hOanOTTmp6NyUhkznpzRUxpgaBYGhLgc9E3gAWGqt\nfdw7YLq19vow2l4K9DXGJAE7gSuBH5Srsw0YD3zq5RzsD1Se7yEcPYaSe/k7NF/8AM02vAIf/R7W\nvO5yeXUbRJPYGP56YV9+9PoG1uw9ylVPLuFfPzqDTq1DXGIm0hDs+hLWvunuV+s/yc2OWVXWQt5+\nKDjikp8XeUvhMSjKd0FewVHY/oW7nPPQ9hP375IGp53rJnzpfaabqTccTVvB4B+4Zf8mWDUXjmZB\nXAtXFtcC4rzHpi0hrqW7N09pGERERERqXdAg0Fq71BhzPvATY8x84H9xk7mEZK0tMsbcihtJjAWe\nsdauMcb82CufDfwBeM4Y8zVupPF/rbXZ1X85YJu14dg5f6TZyB/Amz+FPavgyXFw1s/h7NuJbxbL\nrIv68eP/bGLd7sNc/dQSXp4+moRWTWtyWJHIKDwGq/8Ny56GncvLtr/1SzcRyoDvwoDvQaf+le9v\nrUuDkLmwbDm8I/zjt+wIp46DU8fDqedC6y41ejmAS+sw7s6atyMiIiIi1RJyYhhvopa/GmPmAY9U\npXFr7dvA2+W2zfZ7vguYUJU2w3bquS5/2Ef3wBdPwII/w7r/EDvmD7TtPJh//nAkVz6xmA17c7nm\nmSXMuXE0bVvEhW5XpC7s3wTLnnGXTx7Pcduat4W0yyF3j7s8c+dyt3z0e+jQtywgbNXRXXLpC/rK\nj+Y1awPN20FcczfRSpMW3vMWbj2uhZvc5bTzoOsgiKl476yIiIiIRK+wZwe11u4ErohgX2pfs3iY\n9Gc3s+Abt0LWeuJfv4LCpO/QNGkE89L7cMsHeSzeWcz1z37BP39YfvJSkTDkH4FvP4Stn5G49zB0\nyHajXR1OC28WTICSYneZ5vYlsPRp2Dy/rKz7UBjxQ3f/nW9Gy4I8d7nm+rfgm3fcLJyfzXRLeS0S\noPcYSBwLiWdB54EK7EREREQasVATwzQBfghMAXw3IO0E3gCettYWBto3Uvr37283bNhQ9R0Lj0HG\n/fD5/0G5LBT5xLG5pBsHWiUxcsSZxHUdCF3T3DTympBCKnNkH2x4xwVhmzOgOL/yevFdXTDY8TT3\nGNsMjuyFI3tcG7ne49EssMVl+zVpAWmXwvAfQo+hwftSXATbFrm+bHjLBaW9z3QBX+JZ0DlFQZ9o\ncgNpsHRuSkOm81MaqkhPDPNPIAe4G5fnD6AncB3wAjC1ugeuc3Et4Pzfu6TV2xZB1gbIWg9ZG2h2\naDvJMdvg2DZY8EnZPs3auGCw6yD32G0QdBpQIQehRAFrax7Q79/kAq31b7kRu9LbYw30GgWnnc/W\nTevp3Srf1d2/yQv29sDWhaHbb9EeEnq75OqnX+lG8MIR2wSSxrrlggeq++pEREREpJEIFQQOs9b2\nK7dtB7DYGPNNhPoUVJPY4LMR5uW4FIMt27WpvLxJF+hzMS2H+pXn57L9my95+t/v0rVwK2Pi95Aa\nux1zdK+7t8p/SvvYppS070dxz1HEpUyC3me5+6n8HNqdBUDbbp1OPLi1sP9bN3KUtQF6joDTxrt7\nuKr4OmojDUVdqJUUEcVFbhbKqgRxBUddsL/5E9jyCez5Gtr0hE79XCDfqT907O/W/YMta93oXPYG\nyPrGe9wA2d+4ETyf2KbQJ93dh9fvgtIJU7bYDHr7fjEsKYZDO9ylmvs3uc++uBBad3XpFeK7uP3i\nu0CrztBEkxOJiIiISOSFCgIPGGMuB171JojBGBMDXA4cjHTnKhMb4rK2gjyXkDpQ8FRpebPW9Eo7\nmys6DObKv3/OAznF9Exowf2TOzO29R7Y8xXsXuUCiQObiMlaTUzWavjyaXfZXuJZ0Pd8N5FGh1Mp\nPO53aeDRbBf0bZ4PmzJOnJlx6ZOAcbM89p3g2ug2GGJiQr4OglzGG7UKjrpgy2+Ulqz1bnbLuFbQ\noQ+0P9Xdb+f/2LI9lBS5SVJ8Qd/2L6Ck3NXKh7a55dsPT9we39UFgwV57vj5hyvvX7O20G+CC/xO\nOy/0/X4xsW5kL6G3qy8iIiIi0gCECgKvBB4EZhljDuLSOLQF5ntlJ5WB3dvw5CUD+O0Hm/km+xjX\nzN3KhIFduGvyzfQY6yWVz88l9+sM4nZ8RvM9n7kUFN9+4BaAhCRadR6NbdIc3lvmAkd/LTu4EaRO\nybDtczd7485lbsn4oxsR6ns+cR1HUdR5MNhO4Y+AFRXA7pWw9XM3CrZvrZvl8ZQz3D1i3YdWGLWs\nsrwDXoC2Hvath4Nb3L1uSWe7YzRvG2Zf82HnCvf6d3zh2svZFrh+QS7s/sot5TVv60bdCo74bTTQ\nfQgknQN9zoEew91Inn+A6Rvt812y6dMioWyUsGN/b9Swn8tbp3vrRERERCTKhcoTmIl3358xpoO3\nbX/ku1V/+ndqyYtXpvD6lqM88sE3vL92Lwu/zean4/tyw1lJxDVrTXH3URR3H0Xz7g9A7l7Y9JEb\nXdr0MRzcQouDW8oabNLcBWF90l2+tS5pJwYS+UdgywLY+D5s/MCNFK6cQyvmuPK4Vm5SkY79XBqA\njt5S1NxNcLNpvgv4tn4OO5a5ZN/+cvxGvmKbulHHU0bDKWfCKaPKAqj8XBdEFRx1fSrIdY9H97lg\nL2udC5z8L4n02fg+LJ4FJtYLvM52yymjy+oUFbiRusyFkPmpG6kr39eYOBdQdupfdslmpwFuxC//\nCBzw7rM74XEzHD/k9u/YryzoSzyr4j11zdu49y75e2XbSkrc6GD2Ri81Qn93ea4mBBIRERGRk1TI\nFBHGmLbARKCHt74TeM9amxPhvtWbJjGGG8f24XuDuvP7/67h7a/3cP876/n3ip3cOyWVvv63brXu\nAoN/4JaSYti5gqPLXsOUFNBy8HddIBTXIvDBmsXDgElusRb2rYON71O49l1iD2wg5viBSkfAOmDA\nxJw4oyScOPLXJdWNem39HLYtdiOD2xa5hUcA44LU8sFYMHEtveAs2T0mJMLe1S6Q3bGsbFRz4cMQ\n25Q2HU93wWH2VxWP0ynZBWu9z3DBcfukwJPuNGkGrTpAr5EnbrfWm1nTVi+ReUyMew0JiVXfV0RE\nREQkCgUNAo0x1wJ3Ae/jUkMAjAP+aIy5x1r7fIT7V6+6tm3OrKuGkbFhH797Yw0b9uZy+exFXDSw\nIz89syftyu8QEwu9RnCsqBsALas6aYsx0GUgdBnI0T4/AKBduyZuQpHsb7zFe35gC1Di7iHsfaYL\n/E45A+LLTUbTNRXSLnPPjx2EbUvKAsGdK7zAzEDTeBeQ+j82jXf323XsB529oK/tKRUviUy52D3m\n58LWRe6evC0LYM/XNN27tKyeL+hLPMvlrSvf1+owBuI717wdEREREZFGItRI4P/DzRB6wqifMSYB\nWALUeRCYXxg8NWG77sEDguqUp/fvzPs/78CsjE3MztjEG2uz+eDbg1x3Zi7Tx/YhodWJszrWxoyd\nJ/Sj5cgKI2CmuNDNNOlLHh6OFgnQf6JbwN2XV1zoRvdq4163Zq3dxCn9Jrj1vANuZlVb4i4/rY2g\nT0REREREaiTUN39DWTI0fyVeWaPRPC6WX5zfj3d/NpbxAzqTV1DM4xmbOOvBj3novQ3k5BXUbYdi\n4xgjVScAACAASURBVKoWAFamSTM36hepyU5atofkC2HgRQoARUREREQaiFAjgfcBK4wx7wPbvW2n\nAOcDf4hkxwKpcZ7AGpb36RTP/13cn1W7uvP44p188k0Wf5v/Lf/4PJMbzkrihrOSIMcNnFbIE1gF\nyhMoIiIiIiKREHQIyFr7D2A48AmQ7y0ZwHBr7XOR7lxlwskT6MuxF4lyX50B7eL4xw0jeXXGGZx1\nWkdy84v460cbGfvgxzz++TYOHs4L/kJCCNkPa0/OXIEiIiIiIhJRIWcHtdYeBF6ug75EpWG92/PC\njaNYsnk/j3z4DYs3H2D20j28tCqLm88t4tozEmnRNPjopYiIiIiISF0JOqxmjDlgjHnKGDPeGCVO\nC2ZUnw68PP0MXrxpFIO7tuJQfjH3v7Oec/48n38u3kpBUUl9d1FERERERCTkxDBZwErg98AOY8xf\njTGjQ+zTqJ15akeenHwqj16QREr3NuzLzee3r69m/MMZvLp8B8UluoRTRERERETqT6gg8Ki19m/W\n2jHAGbhcgbOMMZuNMX+MfPeikzGGM09pw39uPYtZVw3l1E6t2H7gGL985SsmzlzAu6t3Y3U/n4iI\niIiI1INQ9wSWXgJqrd0G/An4kzFmADA1kh0LpD7yBFa1jv+MnZPSujFhYBdeX7mLRz74ho37jvDj\nF1YwqGdbfjq+L+cO6ExlV9qGPEaUzLYZLf0UEREREWksQo0Ezq9so7X/v717j7KyvO+///7OARiO\ngwznM4IIqKiQ4LFKlHhK1VSNJtGcdFlbrTb5mTR52qdtkierNWnzM4kaY0w0sVGbmFg1pSaNBs9n\nEBRQQY4D6DAgAwxnuJ4/9gYnCDObgc3em/1+rcWa+3DtfX9nrWspX+77vj7pjZTS1/NQzyGpqrKC\niycM4vEbT+MbF4yjd7eOzKpv4sqfvcx533+a/3ltBTt8TFSSJEnSQdBWRMSXDlYhucolJ3Bnxl4+\nzucypmnFSppWrPzA8Y5VlXzmxGE8+eXJ/MN5Y+jTrSNzVqzlr34xnbNufpKHXl3Gtu07crpG48L6\nXVmBxaxxwdJdWYGSJEmSCq+t1UEjIj4REZdkt8+IiO9HxF9HRFt3EfOiWHICWxuzddNmtm7avNfz\nNR0querUETz5lcl884JxDOjRiXkN67nh/lc587tP8MuXl9K8boM5gZIkSZIOuLbeCbwV6AN0AC4A\nOgIPA+cBo4Eb8lrdIa5TdSVXnDiMSz80hAdn1HPrH99m0aoNfOWBWfTv1oFTh9UysG8Tvbt1pK5r\nB+q6daR3147Ude1Y6NIlSZIklai2msBTU0pHR0Q18A7QP6W0JSLuA6bnv7zy0KGqgks/NISLjh/E\nI7OWc8vj83l7ZTO/fK0BXmvY42e6VFdwRK8a/u3TPTm8d9eDXLEkSZKkUtVWE7gNIKW0NSJeSilt\nye5viwjTzw+wqsoKPn7cIM4fP5D/eX4eS9ZsZn1FNY3rN9O4fkvm57rMdvPWHcx4p5kLbnmGf//E\neM4a16/Q5UuSJEkqAW01ge9ERNeU0vqU0tk7D0ZEP2BLfksrX5UVwSnDaoE9R0WklJg3ZxHffmYZ\nf1jQxF/e8wrXTR7JF6ccQWXFB+MmJEmSJGmnVpvAlNI5ezm1DvjYgS+nbaWWE9herV0jIjhi3HB+\nPHYYdzy5gJsefYNb/jif15Y18b3LjqW2c4f9vv6BYk6gJEmSVFzatcJnSqk5pbTnl9V00EQEf3na\n4dxz5SR6dq7mibdWcv4tzzBneesRF5IkSZLKV7tjHiKiIAvDlHJO4L7Yl5zAk0fW8cjfnMLRA3uw\nZPUG/uKHz/DQq8v26/oHijmBkiRJUnFpdxOYUjr+QBaSq0MhJzAXbdaxW07goJ6d+dU1J3LxhEFs\n2rqDG+5/la8/MpsFK9ezaev2/apFkiRJ0qGjrYVhdomIwwBSSqvzV472R6fqSr5z8TGMH1zLNx6Z\nzV3PLOKuZxYB0KdbRwb1rGFQz85/8vPogT3o2aV43iGUJEmSlF+tNoERMQT4NnAGsCZzKLoDjwNf\nTSktynuF2icRwRUnDGVs/+7c/Ie3WLSqmeVrNtGwbjMN6zYzfcmaPxnfsaqCiyYM4spThps3KEmS\nJJWBtu4E/idwM/DplNJ2gIioBC4B7gdOyG95aq8JQ3tyz5WTANi2fQfvrttM/eoN1L+3MftnAwsb\nm3l58Xvc+8IS7n1hCWeO6cNVp45g0vDDiDBqQpIkSToUtdUE1qWU/rPlgWwzeH9EfDN/ZelAqqqs\nYGBtDQNra5i027n5Dev4ydML+fX0ZfxhbgN/mNvAMYN6cNWpIzj3qH5UVbb7tVFJkiRJRaitJvCV\niLgN+Bmwc4nHwcBngRn5LGxvzAnMXuMA5e+N7NONf/mLY/g/Hx3NPc8t5p7nFzOrvonr75vBTbU1\nXH7CUE4e2Ysx/btT3Y6G0JxASZIkqbi01QR+BrgS+DowMHusHngE+Eke69JBVte1I1+ccgR/dfrh\n/Gb6Mu58agELGpu56dE3AOhUXcExg2o5fkhPjh9Sy/FDe1LXtWOBq5YkSZK0r1ptAlNKW4AfZv/s\ns4g4G/geUAncmVL61z2MOZ3Me4fVQGNK6bRWC84hJxCgc233vJzPZczOjMAe/Xu3Wmtr2rrGzozA\nA3HXsaVO1ZV8atIQLvvQYB5/o4Gpr69gxpI1LGxs5sWFq3lx4fuLww45rDMThvbknKP6ccaYvlRW\nfPA9wp0Zgd4RlCRJkopDW6uDXp1SuqM9Y7ILyNwKTCFz9/CliHg4pTSnxZha4Dbg7JTSkoho81nM\nXHICYe/N0/6ez2XM/mYE5lRHi4zAfKioCM4c25czx/YFYHXzFmYseY/pS95j+uI1zKxfw5LVG1iy\negMPzljGwNoaPn3CEC6dOJhe3iGUJEmSilZbj4N+NSIaWzkfwA3AnhrFDwPzU0oLACLifuACYE6L\nMZ8CfpNSWgKQUmrItXAdXId16cAZY/pyxphMU7ht+w7efHcdT89r5N4Xl7B41Qa+/eib3Py/8/jY\nMf35zEnDOHZwbYGrliRJkrS7SK3cUYqIu3L4jqaU0t/u4bMXk7nDd1V2/wpgUkrpuhZjdj4GOg7o\nBnwvpfTzPXzX1cDVAGOOGD3hth/dvtdihvcZAMDChuV5OZ/LmDGDhgEwt37RXr+jLW1dY9zgEQDM\nXrqg3dc4UHakxOuN23lsyTZmrdzOzhk1vHsFVxw3gCkjevD2ikUFrPDgWb9+PV27mreo4uPcVLFy\nbqqYOT9VrCZPnvxKSmliez/fahO4P3JsAm8BJpIJo68BngPOSym9tbfvPeaoo9Ks11/f63XXLM/c\nTNzb6pr7ez6XMQfifb02r1Gk79otWbWBX7ywmP98eSlrNmRWcu1UFUwaUcepo+o4eWQdR/brdsjm\nEE6bNo3TTz+90GVIH+DcVLFybqqYOT9VrCJiv5rAth4H3R/LyMRJ7DQoe6ylemBVSqkZaI6IJ4Hx\nwF6bQBW3Ib0687Vzx/DFKUfwyMzl3PXEPOas3MgTb63kibcyC+bUde3AySMzDeEpI+sYUFtT4Kol\nSZKk8pHPJvAlYFREDCfT/F1G5h3Alh4CbomIKqADMAn4v619qTmB2WsU2R3A3XWqruSSiYO5ZOJg\nGtZt4tn5q3hqXiPPzG/knbWbeOjV5Tz0auZR10E9a+hRU01NdSWdqivpVF2R/ZnZrqmu5KiBPfjI\nkX3o1qm6wL+ZJEmSVNry1gSmlLZFxHXA78hERPw0pTQ7Iq7Jnr89pTQ3Ih4FZgE7yMRI7P1ZT5Wk\nPt06ceFxA7nwuIGklHh7ZTNPz1vJ0/NX8fyCVdS/t5H69za2+T0dKis4dVQdZx/Vjylj+1LbucNB\nqF6SJEk6tOTUBEbEDcBdwDrgTuA44Ksppd+39rmU0lRg6m7Hbt9t/zvAd3Iu2JxAIH85gQfa7u8u\nRgQj+3RlZJ+ufO7k4WzbvoNFqzawcct2Nm3bzqat27PbO9i0NbO/duNWnnyrkZcWr+axNxp47I0G\nKiuCE0f04uyj+nHWuH707mYshSRJkpSLXO8EfiGl9L2IOAvoCVwB3AO02gTmgzmBWXnOCTxYqior\nGNmn7VW3rvvIKBrWbeL3s9/l0dff4bkFq3h6fiNPz2/k/33odU4+vI4vThnFhKGHHYSqJUmSpNKV\naxO4cynHc4F7so91HprLO6po9enWictPGMrlJwzlveYt/O/cTEP49LzGXQ3hGUf24cazRjOm/96b\neEmSJKmc5doEvhIRvweGA1+LiG5k3uGTCqJnlw58YuJgPjFxME0bt/KTpxZw59MLeeyNBh5/s4Hz\nxw/gS1OOYGivLoUuVZIkSSoqrT9b+b4rga8CH0opbSCzkufn81aVtA961FTzpY+O5smvTOZzJw2j\nuqKCh15dzhn//gR//+BrvLt2U6FLlCRJkopGrk1gAsYC12f3uwCd8lKR1E51XTvyz+eP4/EbT+Pi\nCYPYkRK/eGEJp33nj/zL/8xl6eoNhS5RkiRJKrhcHwe9jczjnx8BvkFmldBfAx/KU117ZU5g9hpF\nnhO4UyHqHNSzM/92yXj+8s9G8O+/f4tHZ7/Dj55YwI+eWMDRA3tw7tH9Offofj4qKkmSpLKUaxM4\nKaV0fETMAEgpvRcRhrSpqI3q243br5jAzKVrMu8Lzn2X15Y18dqyJm569A3GDeiebQj7M7zOhlCS\nJEnlIdcmcGtEVJJ5LJSI6E2BFoYxJzCjVHMCC2H84Fp+8Mnj2LR1O0+8tZKpr63gsbkNzF6+ltnL\n1/Kd373JmP7d+fhxA7h04hB6dK4uWK2SJElSvuXaBH4feBDoExHfAi4G/iFvVbXCnMCsQyQn8GDq\nVF3JWeMy4fKbtm7nqXmNTH1tBX+Y8y5zV6xl7oq1/N//ncfHjx/I504axhF9uxW6ZEmSJOmAy6kJ\nTCn9IiJeAc4gkxl4YUppbl4rk/KoU3UlU8b2ZcrYvmzetp0n3lzJPc8v5ql5jdz7whLufWEJJx3e\ni8+dNIwzxvSlssJYTEmSJB0acmoCI+JwYGFK6daIOB2YEhErUkpr8lqddBB0rKrko+P68dFx/Zjf\nsI6fPbuYX0+v59m3V/Hs26sY1LOGz544jAuOHUBVZQVbt+9gy7YdbN2+g63bU2Z/+w527Eis3LCD\n7TuSTaMkSZKKVq6Pg/4amBgRI4EfAQ8D9wLn5qswqRBG9unGNy88ihvPGs2vXl7Kz59bzJLVG/jW\n1Ll8a2puN7///tlHGXpYZ4bVdWF4XReG9erCsLrODK/rQr/unYiwQZQkSVLh5NoE7kgpbYuIvwBu\nSSn9YOdKodKhqEdNNVedOoLPnzycP77RwN3PLuLVpWuoqgyqKyvoUFlBdXa7urKC6qoKAljY0ETT\n5h3Ma1jPvIb1H/jeUX268q8XHc2EoYcd/F9KkiRJYt9WB/0k8Bngz7PHCrKEojmB2WuYE3hQVFYE\nZ47ty5lj++Y0ftq0aUw88RQWNTazaFUzixqbWdCY+fn2ymbmNazn4tuf4wsnD+fGj46mpkPrq91K\nkiRJB1quTeDngWuAb6WUFkbEcOCe/JUlla6uHas4amAPjhrY40+Ob9q6ne89No87nlzAT7K5hTdd\ndAyTRvQqUKWSJEkqR63nLWSllOaklK5PKd0XET2Bbimlm/Jc2x7lkhO4M2MvH+dzGdO0YuWurMD2\nausajQvrd2UFFrPGBUt3ZQWWu07Vlfzd2Ufy4F+fxOi+3Vi0agOX3vE8//TQ6zRv3lbo8iRJklQm\ncmoCI2JaRHSPiMOA6cCPI+K7+S1tz3LJCdyZsZeP87mM2bpp835nBbZZR0pmBZaoYwbV8sjfnML1\nZ4yiqiL42XOLOevmJ3lmfmOhS5MkSVIZyKkJBHqklNYCfwH8PKU0CTgzf2VJh7YOVRV8acoRPHTd\nyYwb0J369zby6Ttf4Gu/mUXDutb/EUKSJEnaH7k2gVUR0R/4BPDbPNYjlZVxA3rwX9eezI0fPYLq\nyuC+F5dy6k1/5B8fep1lazYWujxJkiQdgnJtAr8B/A54O6X0UkSMAOblryypfFRXVnDdR0bx39ef\nykfH9mXzth38/LnFnPbtP/KVB2aysLE5p+/Zun0Hry9r4s131uW5YkmSJJWynFYHTSn9CvhVi/0F\nwEX5KkoqR0f07cYdn5nIm++s47Zp83lk5nJ++XI9D7xSz3nHDODayYdzZL/uu8avaNrIjCVrmLHk\nPV5duoZZ9U1s3rYDgPGDenD5CUP58/ED6FRtDIUkSZLel1MTGBGDgB8AJ2cPPQXckFI66MtTmhOY\nvUaJ5O+VSp3FZHS/bnzvsuP44plH8MNpb/ObGfU8MnM5j8xczhlH9qG6soIZS9/j3bUfXHxoeF0X\n3tuwhZn1Tcx8YBbfmjqXT0wczKcnDWFory4F+G0kSZJUbHLNCbwLuBe4JLt/efbYlHwUJQmG1XXh\npouP4YYzR3HHkwu478UlPPZGw67z3TtVceyQnhw7uJbjhtRy7KBaenbpwKat23lk5nLueX4xs+qb\nuOPJBfz4qQWcdkRvrjhhKKeP7kNlRRTwN5MkSVIh5doE9k4p3dVi/+6I+Nt8FNSWXHICATrXds/L\n+VzG7MwI7NG/d6u1tqata+zMCDwQdx3zaWdGoHcE229AbQ3/fP44rp08kodnLqdHTTXHDq5lRF0X\nKvbQzHWqruSSiYO5ZOJgXl26hnueW8wjs5Yz7c2VTHtzJYN61nD9R0Zx0YRBNoOSJEllKNcmcFVE\nXA7cl93/JLAqPyW1LpecQNh787S/53MZs78ZgTnVYUZg2endrSNXnjJ8nz5z7OBajh1cy9+fN4Zf\nvryU/3h+MfXvbeQrv57FT55eyN+dM5rJo/sQYTMoSZJULnJdHfQLZOIh3gFWABcDn8tTTZIOsMO6\ndOCa0w7niS9P5uZLj2VgbQ1vvruOL9z9Mp/88fPMXLqm0CVKkiTpIMmpCUwpLU4pnZ9S6p1S6pNS\nuhBXB5VKTmVFcOFxA3n8xtP4h/PG0KOmmucXrOaCW5/hununs3hVbnEUkiRJKl253gncky8dsCok\nHVQdqyq56tQRPPnlyfzlaSPoUFXBb2et4MzvPsE/PzzboHpJkqRDWK7vBO6JLxFJJa5H52q+ds4Y\nPnPiML77+7f4zYx67n52EXc/u4jhdV048fBenHR4L04c0YteXTsWulxJkiQdAPvTBBZkZRJzArPX\nKJHVNkulznI3sLaGf//EeK48ZTg/eHweT81rZGFjMwsbm7n3hSUAHNmvGycdXsdJh/fiwyMOo3un\n6gJXLUmSpPZotQmMiHXsudkLoCYvFUkqmLEDuvPDyyewbfsOXlvWxLNvr+K5t1fx0qLVvPHOOt54\nZx0/fWYhACN6d2H8oFqOGdSD8YNrGdu/O52qW49wkSRJUuG12gSmlLodrEJyZU5ghjmByqeqygqO\nG9KT44b05NrJI9m8bTszlqzh2bdX8ez8RmbVN7FgZTMLVjbz4Ixlmc9UBKP7deOYbGM4sk9Xhtd1\noVeXDkZQSJIkFZH9eRy0IMwJzDInUAdRx6pKThjRixNG9OJLU45gy7YdvPHOWmbWNzFr6Rpm1Tcx\nr2Eds5evZfbytdz34vuf7daximF1XRhW14XhvTrv2h7Zp6uPlEqSJBVAyTWBkgqvQ1VF9o5fLZww\nFIDmzdt4fVkTs+qbeH150653Ctdt2sZry5p4bVnTB75n8GE1jOvfg7EDujNuQHfGDuhOv+6dvHMo\nSZKUR3ltAiPibOB7QCVwZ0rpX/cy7kPAc8BlKaUH8lmTpPzo0rGKSSN6MWlEr13HUkqsbt7ColXN\nLGzcwKLGZhauambhymbmr1zP0tUbWbp6I4/OfmfXZw7r0oGx/btzRN9u9OvRkT7dOtGnW0d6d8ts\nd6+pskmUJEnaD3lrAiOiErgVmALUAy9FxMMppTl7GHcT8Pt81SKpMCKCXl070qtrRyYMPexPzm3b\nvoO3VzYzZ0UTs5etZc6KzKOkq5u38PT8Rp6e37jH7+xQVUHvrh3p270jJx1ex0UTBjG8rsvB+HUk\nSZIOCfm8E/hhYH5KaQFARNwPXADM2W3c3wC/Bj6Ux1okFZmqygpG9+vG6H7d+PhxmWMpJZY3bWLO\n8rW8vXI9DWs3s3L9ZhrWbmLlus2sXLeZdZu3sWzNRpat2cj0JWu45Y/zOX5ILRdNGMTHjhlAjxrf\nM5QkSWpNPpvAgcDSFvv1wKSWAyJiIPBxYDI5NoHmBGavUSKrbZZKnSoOEcHA2hoG1tYwhb57HLNh\nyzZWrtvM4lUbeGTmcqa+toLpS9Ywfckavv7IHKaM7cvFEwZx6sg6qipbX0hKkiSpHBV6YZibgb9L\nKe1o7R2fiLgauBqgd+/eTJs27eBUJ+2j9evXOz8PovN6w5l/1pGX393GM8u3MXfVDv571gr+e9YK\nenQMTuxfxakDqxjYzWbQuali5dxUMXN+6lAVKU9RAxFxIvDPKaWzsvtfA0gp/UuLMQvJBM8D1AEb\ngKtTSv+1t+8dN3Zsmj1n9ydK32dOYHEpt5zAadOmcfrppxe6jLK1bM1GHpxez6+nL2NhY/Ou4+MH\n9eDiCYM4f/xAenQuz8dFnZsqVs5NFTPnp4pVRLySUprY3s/n807gS8CoiBgOLAMuAz7VckBKafjO\n7Yi4G/htaw0gmBO4izmB0gcMrK3huo+M4trJI5m+ZA0PvFLPb2cuZ2Z9EzPrm/jmb+cyZVzmcdE/\nG9WbygpXGZUkSeUnb01gSmlbRFwH/I5MRMRPU0qzI+Ka7Pnb83VtSeUtIpgwtCcThvbkn/58LL+b\n/Q4PvFLP0/Mbdz0u2rd7Ry48biDnHNWfYwb2oMKGUJIklYm8vhOYUpoKTN3t2B6bv5TS5/JZi6Ty\n1Km6kguOHcgFxw5k+ZqNPDhjGb96eSmLVm3gR08s4EdPLKB3t46cOaYPZxzZl5NH1lHTobLQZUuS\nJOVNoReGkaSDZkBtDddOHslfn344ryx+j4dnLuexuQ0sW7OR+15cyn0vLqVTdQWnjOzNlLF9mHxk\nH2prOrC6eQurmjezan3Ln1tYtX4zq5u38G+XjKe2c4dC/3qSJEk5sQmUVHYigonDDmPisMP4+vmJ\nuSvW8Ye57/KHue8yq75p13auGtZttgmUJEklo+SaQHMCs9cokdU2S6VOla+IYOyA7owd0J3rzxjF\nu2s38djcBv4w912ent/I9h2Jw7p0oFeXDvTq2oFeXTpmf3agV9eO9OrSgX49OhX615AkScpZyTWB\nkpRPfbt34lOThvCpSUPYtn0HFREuGiNJkg4pJdcEVlW2vmCDOYHFpdxyAnVoqao0ZF6SJB16Su5v\nOLnkBO7M2MvH+VzGbN20eb+zAtusIyWzAiVJkiTts5JrAiVJkiRJ7WcTKEmSJEllxCZQkiRJksqI\nTaAkSZIklZGSWx3UnMDsNUpktc1SqVOSJEkqF94JlCRJkqQyUnJNYC45gTsz9vJxPpcxTStW7soK\nbK+2rtG4sH5XVmAxa1ywdFdWoCRJkqTCK7km0JzALHMCJUmSJLVDyTWBkiRJkqT2swmUJEmSpDJi\nEyhJkiRJZcQmUJIkSZLKiDmB+3g+lzHmBL6vVOqUJEmSyoV3AiVJkiSpjJRcE2hOYIY5gZIkSZLa\no+SaQHMCs8wJlCRJktQOJdcESpIkSZLazyZQkiRJksqITaAkSZIklRGbQEmSJEkqI+YE7uP5XMaY\nE/i+UqlTkiRJKhfeCZQkSZKkMlJyTaA5gRnmBEqSJElqj5JrAs0JzDInUJIkSVI7lFwTKEmSJElq\nP5tASZIkSSojNoGSJEmSVEZsAiVJkiSpjJgTuI/ncxljTuD7SqVOSZIkqVzk9U5gRJwdEW9GxPyI\n+Ooezn86ImZFxGsR8WxEjM9nPZIkSZJU7vLWBEZEJXArcA4wFvhkRIzdbdhC4LSU0tHAN4E72vpe\ncwIzzAmUJEmS1B75vBP4YWB+SmlBSmkLcD9wQcsBKaVnU0rvZXefB9p8jtKcwCxzAiVJkiS1Q6Q8\nNRIRcTFwdkrpquz+FcCklNJ1exl/I3DkzvG7nbsauBpgzBGjJ9z2o9v3et3hfQYAsLBheV7O5zJm\nzKBhAMytX7TX72hLW9cYN3gEALOXLmj3NQ6GUqnzQFm/fj1du3YtdBnSBzg3Vaycmypmzk8Vq8mT\nJ7+SUprY3s8XRRMYEZOB24BTUkqrWvveY446Ks16/fW9nl+zvAHY+8Iq+3s+lzE7H9PcnwVi2rxG\n9hHLYl94pVTqPFCmTZvG6aefXugypA9wbqpYOTdVzJyfKlYRsV9NYD5XB10GtPyb/6DssT8REccA\ndwLntNUASpIkSZL2Tz7fCXwJGBURwyOiA3AZ8HDLARExBPgNcEVK6a081iJJkiRJIo93AlNK2yLi\nOuB3QCXw05TS7Ii4Jnv+duAfgV7AbREBsK2t25rmBGavUSKPV5ZKnZIkSVK5yGtYfEppKjB1t2O3\nt9i+CvjAQjCSJEmSpPzIa1h8PpgTmGFOoCRJkqT2KLkm0JzALHMCJUmSJLVDyTWBkiRJkqT2swmU\nJEmSpDJiEyhJkiRJZcQmUJIkSZLKSF4jIvLBnMDsNUokf69U6pQkSZLKhXcCJUmSJKmMlFwTaE5g\nhjmBkiRJktqj5JpAcwKzzAmUJEmS1A4l1wRKkiRJktrPJlCSJEmSyohNoCRJkiSVEZtASZIkSSoj\n5gTu4/lcxpgT+L5SqVOSJEkqF94JlCRJkqQyUnJNoDmBGeYESpIkSWqPkmsCzQnMMidQkiRJUjuU\nXBMoSZIkSWo/m0BJkiRJKiM2gZIkSZJURmwCJUmSJKmMmBO4j+dzGWNO4PtKpU5JkiSpXHgnUJIk\nSZLKSMk1geYEZpgTKEmSJKk9Sq4JNCcwy5xASZIkSe1Qck2gJEmSJKn9bAIlSZIkqYzYBEqSw/Dr\niAAACOVJREFUJElSGbEJlCRJkqQyYk7gPp7PZYw5ge8rlTolSZKkcuGdQEmSJEkqIyXXBJoTmGFO\noCRJkqT2KLkm0JzALHMCJUmSJLVDXpvAiDg7It6MiPkR8dU9nI+I+H72/KyIOD6f9UiSJElSuctb\nExgRlcCtwDnAWOCTETF2t2HnAKOyf64GfpiveiRJkiRJ+b0T+GFgfkppQUppC3A/cMFuYy4Afp4y\nngdqI6J/HmuSJEmSpLKWzyZwINByRZD67LF9HSNJkiRJOkBKIicwIq4m87gowOaIeL2Q9UitqAMa\nC12EtAfOTRUr56aKmfNTxWr0/nw4n03gMqBlUvig7LF9HUNK6Q7gDoCIeDmlNPHAliodGM5PFSvn\npoqVc1PFzPmpYhURL+/P5/P5OOhLwKiIGB4RHYDLgId3G/Mw8JnsKqEnAE0ppRV5rEmSJEmSylre\n7gSmlLZFxHXA74BK4KcppdkRcU32/O3AVOBcYD6wAfh8vuqRJEmSJOX5ncCU0lQyjV7LY7e32E7A\ntfv4tXccgNKkfHF+qlg5N1WsnJsqZs5PFav9mpuR6cMkSZIkSeUgn+8ESpIkSZKKTEk1gRFxdkS8\nGRHzI+Krha5H5SsiBkfEHyNiTkTMjogbsscPi4j/jYh52Z89C12rylNEVEbEjIj4bXbfuamiEBG1\nEfFARLwREXMj4kTnp4pBRHwx+//01yPivojo5NxUoUTETyOioWU0XmvzMSK+lu2R3oyIs9r6/pJp\nAiOiErgVOAcYC3wyIsYWtiqVsW3A/0kpjQVOAK7NzsevAo+llEYBj2X3pUK4AZjbYt+5qWLxPeDR\nlNKRwHgy89T5qYKKiIHA9cDElNJRZBY1vAznpgrnbuDs3Y7tcT5m/w56GTAu+5nbsr3TXpVMEwh8\nGJifUlqQUtoC3A9cUOCaVKZSSitSStOz2+vI/CVmIJk5+bPssJ8BFxamQpWziBgEnAfc2eKwc1MF\nFxE9gD8DfgKQUtqSUlqD81PFoQqoiYgqoDOwHOemCiSl9CSwerfDe5uPFwD3p5Q2p5QWkkle+HBr\n319KTeBAYGmL/frsMamgImIYcBzwAtC3RdblO0DfApWl8nYz8BVgR4tjzk0Vg+HASuCu7OPKd0ZE\nF5yfKrCU0jLg34AlwAoy2dW/x7mp4rK3+bjPfVIpNYFS0YmIrsCvgb9NKa1teS4bgeLyuzqoIuJj\nQENK6ZW9jXFuqoCqgOOBH6aUjgOa2e3xOuenCiH7btUFZP6hYgDQJSIubznGualisr/zsZSawGXA\n4Bb7g7LHpIKIiGoyDeAvUkq/yR5+NyL6Z8/3BxoKVZ/K1snA+RGxiMxj8x+JiP/AuaniUA/Up5Re\nyO4/QKYpdH6q0M4EFqaUVqaUtgK/AU7Cuanisrf5uM99Uik1gS8BoyJieER0IPPy48MFrkllKiKC\nzDstc1NK321x6mHgs9ntzwIPHezaVN5SSl9LKQ1KKQ0j89/Jx1NKl+PcVBFIKb0DLI2I0dlDZwBz\ncH6q8JYAJ0RE5+z/488g876/c1PFZG/z8WHgsojoGBHDgVHAi619UUmFxUfEuWTedakEfppS+laB\nS1KZiohTgKeA13j/vav/h8x7gb8EhgCLgU+klHZ/qVc6KCLidODGlNLHIqIXzk0VgYg4lsyiRR2A\nBcDnyfyjtPNTBRURXwcuJbMC+AzgKqArzk0VQETcB5wO1AHvAv8E/Bd7mY8R8ffAF8jM379NKf1P\nq99fSk2gJEmSJGn/lNLjoJIkSZKk/WQTKEmSJEllxCZQkiRJksqITaAkSZIklRGbQEmSJEkqIzaB\nkqSiEBHbI+LVFn++egC/e1hEvJ7DuLsjYmFEXJPdvyYiXsvW83REjN1LvW3m1kbG9yNifkTMiojj\n9zLuuuyYFBF1+/r5FuN/ERGrI+LitmqTJJWXqkIXIElS1saU0rGFLgL4ckrpgez2vSml2wEi4nzg\nu8DZ2XP7Wu85ZAJ8RwGTgB9mf+7uGeC3wLR2fh6AlNKnI+LufahPklQmvBMoSSpqEbEoIr6dvSP3\nYkSMzB4fFhGPZ++KPRYRQ7LH+0bEgxExM/vnpOxXVUbEjyNidkT8PiJq2rp2Smlti90uwP6E614A\n/DxlPA/URkT/PVxzRkppUa6fj4guEfHf2d/19Yi4dD9qlCSVAZtASVKxqNntcdCWzUxTSulo4Bbg\n5uyxHwA/SykdA/wC+H72+PeBJ1JK44HjgdnZ46OAW1NK44A1wEW5FBUR10bE28C3getbnOoUEdMj\n4vmIuLDF+G9k7xrubiCwtMV+ffZYrvb2+bOB5Sml8Smlo4BH9+E7JUllyCZQklQsNqaUjm3x5z9b\nnLuvxc8Ts9snAvdmt+8BTsluf4TMo5KklLanlJqyxxemlF7Nbr8CDMulqJTSrSmlw4G/A/6hxamh\nKaXjgU8BN0fE4dnx/5hSavMdwQPoNWBKRNwUEae2+H0lSdojm0BJUilIe9neF5tbbG9n39+Lvx/Y\ndccvpbQs+3MBmff3jmvj88uAwS32B2WP5WqPn08pvUXmjudrwP8XEf+4D98pSSpDNoGSpFJwaYuf\nz2W3nwUuy25/Gngqu/0Y8FcAEVEZET3ae9GIGNVi9zxgXvZ4z4jomN2uA04G5rTxdQ8Dn8mu8nkC\nmUdcV+xDOXv8fEQMADaklP4D+A6ZhlCSpL1ydVBJUrGoiYhXW+w/mlLaGRPRMyJmkbmb98nssb8B\n7oqILwMrgc9nj98A3BERV5K54/dXwL40Wy1dFxFnAluB94DPZo+PAX4UETvI/IPqv6aU5kDmnUDg\n5T08EjoVOBeYD2xoUS8RMRW4KqW0PCKuB74C9ANmRcTUlNJVrXz+aOA72Vq2Zn9fSZL2KlLan4XO\nJEnKr4hYBExMKTUehGvdDfy2RURESTvUfh9J0oHh46CSJL2vCfjmzrD4UhYRvwBOAzYVuhZJUnHx\nTqAkSZIklRHvBEqSJElSGbEJlCRJkqQyYhMoSZIkSWXEJlCSJEmSyohNoCRJkiSVEZtASZIkSSoj\n/z+OBFhg/rx51AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f0751e03470>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "fit_params = {\n",
    "    'opt':         'adam',           # SGD(lr=0.01, momentum=0.9, nesterov=True), \n",
    "    'nepochs':         100, \n",
    "    'patience':        30,\n",
    "    'ploss':           1.5,\n",
    "    'reset':           True,\n",
    "}\n",
    "model_name = 'whatevah'\n",
    "train_network(model, model_name, train_feature, y_train_oh, validation_features, y_val_oh,  **fit_params);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/500 [=====>........................] - ETA: 0s\n",
      "[INFO] accuracy on the test data set: 66.80% [0.80362]\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(test_features, y_test_oh, batch_size=batch_size)\n",
    "print(\"\\n[INFO] accuracy on the test data set: {:.2f}% [{:.5f}]\".format(accuracy * 100, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agora vamos carregar estes pesos para um mesmo modelo colado em uma VGG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replique o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.save_weights('../model1_model_weights.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "flatten_1 (Flatten)          (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 256)               131328    \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 256)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 3)                 771       \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 3)                 0         \n",
      "=================================================================\n",
      "Total params: 132,099\n",
      "Trainable params: 132,099\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<tf.Tensor 'block5_pool_3/MaxPool:0' shape=(?, ?, ?, 512) dtype=float32>]\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.models import Sequential, Model, Input\n",
    "#vgg = VGG16(include_top=False, weights='imagenet')\n",
    "\n",
    "modelvgg2 =  VGG16(include_top=False, weights='imagenet')\n",
    "modelvgg2.layers = modelvgg2.layers[:-4]\n",
    "input = Input(shape=(3, 32, 32))\n",
    "print(modelvgg2.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_vgg_create(modelvgg2):\n",
    "    input = Input(shape=(3, 32, 32))\n",
    "    modelvgg2.trainable = False\n",
    "    x = modelvgg2(input)\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(256, activation='relu')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    output = Dense(3, activation='softmax')(x)\n",
    "    \n",
    "    model2 = Model(inputs=input, outputs=output)\n",
    "    return model2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "number of input channels does not match corresponding dimension of filter, 32 != 3",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-e4020b2d3cb2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_bottom\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel_vgg_create\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodelvgg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-d266de5febe3>\u001b[0m in \u001b[0;36mmodel_vgg_create\u001b[0;34m(modelvgg2)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodelvgg2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodelvgg2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mFlatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/adessowiki/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, **kwargs)\u001b[0m\n\u001b[1;32m    594\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m             \u001b[0;31m# Actually call the layer, collecting output(s), mask(s), and shape(s).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 596\u001b[0;31m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    597\u001b[0m             \u001b[0moutput_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/adessowiki/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs, mask)\u001b[0m\n\u001b[1;32m   2059\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output_tensor_cache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2060\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2061\u001b[0;31m             \u001b[0moutput_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_internal_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2062\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0moutput_tensors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2063\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/adessowiki/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36mrun_internal_graph\u001b[0;34m(self, inputs, masks)\u001b[0m\n\u001b[1;32m   2210\u001b[0m                                 \u001b[0;32mif\u001b[0m \u001b[0;34m'mask'\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2211\u001b[0m                                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mask'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomputed_mask\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2212\u001b[0;31m                             \u001b[0moutput_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_to_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcomputed_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2213\u001b[0m                             output_masks = _to_list(layer.compute_mask(computed_tensor,\n\u001b[1;32m   2214\u001b[0m                                                                        computed_mask))\n",
      "\u001b[0;32m/home/adessowiki/anaconda3/lib/python3.6/site-packages/keras/layers/convolutional.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    162\u001b[0m                 \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    163\u001b[0m                 \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 164\u001b[0;31m                 dilation_rate=self.dilation_rate)\n\u001b[0m\u001b[1;32m    165\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrank\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    166\u001b[0m             outputs = K.conv3d(\n",
      "\u001b[0;32m/home/adessowiki/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36mconv2d\u001b[0;34m(x, kernel, strides, padding, data_format, dilation_rate)\u001b[0m\n\u001b[1;32m   3136\u001b[0m         \u001b[0mstrides\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstrides\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3137\u001b[0m         \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpadding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3138\u001b[0;31m         data_format='NHWC')\n\u001b[0m\u001b[1;32m   3139\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m_postprocess_conv2d_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/adessowiki/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/nn_ops.py\u001b[0m in \u001b[0;36mconvolution\u001b[0;34m(input, filter, padding, strides, dilation_rate, name, data_format)\u001b[0m\n\u001b[1;32m    639\u001b[0m           \u001b[0;34m\"number of input channels does not match corresponding dimension of filter, \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m           \"{} != {}\".format(input_channels_dim, filter.get_shape()[\n\u001b[0;32m--> 641\u001b[0;31m               num_spatial_dims]))\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m     strides, dilation_rate = _get_strides_and_dilation_rate(\n",
      "\u001b[0;31mValueError\u001b[0m: number of input channels does not match corresponding dimension of filter, 32 != 3"
     ]
    }
   ],
   "source": [
    "model_bottom = model_vgg_create(modelvgg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vgg = VGG16(include_top=False, weights='imagenet')\n",
    "#vgg.summary()\n",
    "for k,layer in enumerate(vgg.layers):\n",
    "    if k>10:\n",
    "        print(k, layer.name)\n",
    "\n",
    "input = Input(shape=(3, 32, 32))\n",
    "print(vgg.outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(None), Dimension(512)])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "modelvgg2.output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "del meio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 1, 1)\n"
     ]
    }
   ],
   "source": [
    "print(train_feature.shape[1:])\n",
    "meio = modelvgg2.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "meio = Dense(512,activation='relu')(meio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_meio = Model(inputs=modelvgg2.input, outputs=meio)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/adessowiki/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py:1562: UserWarning: Model inputs must come from a Keras Input layer, they cannot be the output of a previous non-Input layer. Here, a tensor specified as input to \"model_4\" was not an Input tensor, it was generated by layer dense_11.\n",
      "Note that input tensors are instantiated via `tensor = Input(shape)`.\n",
      "The tensor that caused the issue was: dense_11/Relu:0\n",
      "  str(x.name))\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Input layers to a `Model` must be `InputLayer` objects. Received inputs: Tensor(\"dense_11/Relu:0\", shape=(?, 512), dtype=float32). Input 0 (0-based) originates from layer type `Dense`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-e2579afb5c3c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_final\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel_meio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/adessowiki/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/adessowiki/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m   1620\u001b[0m                     'from layer type `{}`.'.format(inputs,\n\u001b[1;32m   1621\u001b[0m                                                    \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1622\u001b[0;31m                                                    layer.__class__.__name__))\n\u001b[0m\u001b[1;32m   1623\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_names\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1624\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlayer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_placeholder\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Input layers to a `Model` must be `InputLayer` objects. Received inputs: Tensor(\"dense_11/Relu:0\", shape=(?, 512), dtype=float32). Input 0 (0-based) originates from layer type `Dense`."
     ]
    }
   ],
   "source": [
    "model_final = Model(inputs=model_meio.output, outputs = model2.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = vgg.output\n",
    "x = Dense(256, activation='relu', name='d1')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(3, activation='softmax', name='d2')(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"flatten_1_input:0\", shape=(?, 512, 1, 1), dtype=float32) at layer \"flatten_1_input\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-42-6cfeb5b3e114>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_somados\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodelvgg2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/home/adessowiki/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/adessowiki/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m   1780\u001b[0m                                 \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m                                 \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m                                 str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m   1783\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m                         \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"flatten_1_input:0\", shape=(?, 512, 1, 1), dtype=float32) at layer \"flatten_1_input\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "model_meio = Model(inputs=modelvgg2.input, outputs=model2.output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=vgg.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential, Model, load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Graph disconnected: cannot obtain value for tensor Tensor(\"flatten_1_input:0\", shape=(?, 512, 1, 1), dtype=float32) at layer \"flatten_1_input\". The following previous layers were accessed without issue: []",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-39-aa93c14e19db>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel_somados\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodelvgg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mW\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/adessowiki/anaconda3/lib/python3.6/site-packages/keras/legacy/interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[1;32m     87\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[0;32m---> 88\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_support_signature\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetargspec\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/adessowiki/anaconda3/lib/python3.6/site-packages/keras/engine/topology.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, inputs, outputs, name)\u001b[0m\n\u001b[1;32m   1780\u001b[0m                                 \u001b[0;34m'The following previous layers '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1781\u001b[0m                                 \u001b[0;34m'were accessed without issue: '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1782\u001b[0;31m                                 str(layers_with_complete_input))\n\u001b[0m\u001b[1;32m   1783\u001b[0m                     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput_tensors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1784\u001b[0m                         \u001b[0mcomputable_tensors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Graph disconnected: cannot obtain value for tensor Tensor(\"flatten_1_input:0\", shape=(?, 512, 1, 1), dtype=float32) at layer \"flatten_1_input\". The following previous layers were accessed without issue: []"
     ]
    }
   ],
   "source": [
    "model_somados = Model(inputs=modelvgg.input, outputs=model2.output)\n",
    "W = model2.load_weights()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "vgg = VGG16(include_top=False, weights='imagenet', classes=3, pooling = 'max')\n",
    "\n",
    "# build a classifier model and put on top of the convolutional model\n",
    "vgg.layers = vgg.layers[:-4]\n",
    "x = vgg.output\n",
    "x = Dense(256, activation='relu', name='d1')(x)\n",
    "x = Dropout(0.5)(x)\n",
    "x = Dense(3, activation='softmax', name='d2')(x)\n",
    "\n",
    "model = Model(inputs=vgg.input, outputs=x)\n",
    "\n",
    "#importa o melhor modelo obtido pelo transfer learning (o arquivo que gerou este modelo é ./my_cifar_transferlearning_original.ipynb)\n",
    "top_model_name = './best_transfer71.model'\n",
    "\n",
    "# Carrego os pesos treinados anteriormente\n",
    "w1, b1, w2, b2 = load_model(top_model_name).get_weights()    \n",
    "\n",
    "# Coloco nas camadas densas finais da rede\n",
    "model.layers[20].set_weights([w1, b1])\n",
    "model.layers[22].set_weights([w2, b2])\n",
    "\n",
    "#set layers from VGG to not be part of the train\n",
    "for layer in model.layers[:17]:\n",
    "    layer.trainable = False\n",
    "    \n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "batch_size=50\n",
    "epochs = 50\n",
    "\n",
    "cb = [\n",
    "      ModelCheckpoint('anything', monitor='val_acc', verbose=0, save_best_only=True, mode='auto', period=1),\n",
    "      ReduceLROnPlateau(monitor='val_loss', factor=0.8, patience=5, verbose=0, mode='auto', epsilon=0.0001, cooldown=0, min_lr=0)\n",
    "     ]\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=[\"accuracy\"])\n",
    "\n",
    "#model.fit_generator(generator        = train_generator,\n",
    " #                                     steps_per_epoch  = int(np.round(train_generator.n / batch_size)),\n",
    "  #                                    validation_data  = validation_generator,\n",
    "   #                                   validation_steps = int(np.round(validation_generator.n / batch_size)),\n",
    "    #                                  epochs           = 200,\n",
    "     #                                 verbose          = 0,\n",
    "      #                                callbacks=cb)\n",
    "h = model.fit(train_feature, y_train_oh,\n",
    "              validation_data=(validation_features, y_test_oh),\n",
    "              epochs=epochs, batch_size=batch_size, callbacks=cb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = model.fit(train_features, y_train_oh,\n",
    "              validation_data=(valid_features, y_test_oh),\n",
    "              epochs=epochs, batch_size=batch_size, callbacks=cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.fit_generator(generator        = gen_train,\n",
    "                                      steps_per_epoch  = int(np.round(gen_train.n / batch_size)),\n",
    "                                      validation_data  = gen_valid,\n",
    "                                      validation_steps = int(np.round(gen_valid.n / batch_size)),\n",
    "                                      epochs           = 2000,\n",
    "                                      verbose          = 2,\n",
    "                                      callbacks=[checkpoint])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCb(TrainingPlotter):\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "fit_params = {\n",
    "    'train_steps':     nb_train_samples / batch_size,\n",
    "    'valid_steps':     nb_val_samples / batch_size,\n",
    "    'opt':             'adam',    # SGD(lr=0.1, momentum=0.9, nesterov=True), \n",
    "    'nepochs':         100, \n",
    "    'patience':        15,\n",
    "    'ploss':           1.0,\n",
    "    'reset':           False,\n",
    "}\n",
    "\n",
    "model_name = 'week05'\n",
    "\n",
    "gen_train,\n",
    "                                      steps_per_epoch  = int(np.round(gen_train.n / batch_size)),\n",
    "                                      validation_data  = gen_valid,\n",
    "                                      validation_steps = int(np.round(gen_valid.n / batch_size)),\n",
    "                                      epochs           = 2000,\n",
    "                                      verbose          = 2,\n",
    "                                      callbacks=[checkpoint])\n",
    "                        \n",
    "                        \n",
    "train_network(model, model_name, train_generator, validation_generator, **fit_params);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    # Aqui os features deixam de ser imagens\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feature.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topmodel.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "modelvgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_feature.shape[1:]\n",
    "train_feat = train_feature.reshape(1700,512)\n",
    "print(train_feat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelvgg.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_build():\n",
    "    img_rows, img_cols = 32, 32 # Dimensões das imagens\n",
    "    #imagens com 3 canais e 32x32\n",
    "    input_shape = (3, img_rows, img_cols)\n",
    "\n",
    "    # Definindo a rede\n",
    "    model = Sequential()\n",
    "    #primeira conv\n",
    "    model.add(Conv2D(32, (3, 3),\n",
    "                     input_shape=input_shape))\n",
    "    model.add(Activation('relu'))\n",
    "    \n",
    "    #segunda conv\n",
    "    model.add(Conv2D(32,(3,3)))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    # Aqui os features deixam de ser imagens\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_classes))\n",
    "    model.add(Activation('softmax'))\n",
    "    return model\n",
    "model_week05 = model_build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_week05.load_weights('../my_cifar_dataplus_model_weights.h5')\n",
    "print(\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_week05.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(model_week05.layers)\n",
    "print(len(model_week05.layers))\n",
    "weights10 = model_week05.layers[10].get_weights()\n",
    "print(weights10[0].shape,weights10[1].shape)\n",
    "weights7 = model_week05.layers[7].get_weights()\n",
    "print(weights7[0].shape,weights7[1].shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "w2, b2 = weights10\n",
    "w1, b1 = weights7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topmodel = Sequential()\n",
    "topmodel.add(layer=keras.layers.Flatten(input_shape=(1,1,512)))\n",
    "# topmodel.add(layer=keras.layers.Dense(units=256, activation='relu', name='d256'))\n",
    "\n",
    "topmodel.add(layer=keras.layers.Dense(units=128, name='d256',))\n",
    "topmodel.add(Activation('relu'))\n",
    "topmodel.add(layer=keras.layers.Dropout(rate=.5))\n",
    "topmodel.add(layer=keras.layers.Dense(units=3, name='d3'))\n",
    "topmodel.add(Activation('softmax'))\n",
    "# topmodel.compile(optimizer=keras.optimizers.SGD(lr=.05, momentum=.9, nesterov=True),\n",
    "topmodel.compile(optimizer='adam',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topmodel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(topmodel.layers)\n",
    "print(len(topmodel.layers))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "topmodel.layers[20].set_weights([w1, b1])\n",
    "topmodel.layers[5].set_weights([wei])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls ../"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w1, b1, w2, b2 = load_model('../my_cifar_dataplus_model_weights.h5').get_weights()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w1, b1, w2, b2 = load_model('../my_cifar_dataplus_model_weights').get_weights()m m  mmm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2.load_weights('../my_cifar_dataplus_model_weights.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "    # Aqui os features deixam de ser imagens\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(n_classes))\n",
    "    model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "topmodel = Sequential()\n",
    "# topmodel.add(layer=keras.layers.Flatten(input_shape=feat_train.shape[1:]))\n",
    "# topmodel.add(layer=keras.layers.Dense(units=256, activation='relu', name='d256'))\n",
    "topmodel.add(layer=keras.layers.Dense(units=256, activation='relu', name='d256', input_shape=(1,1,512)))\n",
    "topmodel.add(layer=keras.layers.Dropout(rate=.5))\n",
    "topmodel.add(layer=keras.layers.Dense(units=3, activation='softmax', name='d3'))\n",
    "\n",
    "# topmodel.compile(optimizer=keras.optimizers.SGD(lr=.05, momentum=.9, nesterov=True),\n",
    "topmodel.compile(optimizer='adam',\n",
    "                 loss='categorical_crossentropy',\n",
    "                 metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#train_features = modelvgg.predict(new_X_train)\n",
    "train_features = modelvgg.predict(new_X_train)\n",
    "\n",
    "print('train_features shape and type',train_features.shape,train_features.dtype)\n",
    "validation_features = modelvgg.predict(new_X_val)\n",
    "print('validation_features shape and type',validation_features.shape,train_features.dtype)\n",
    "test_features = modelvgg.predict(X_test)\n",
    "print('test_features shape and type',test_features.shape,train_features.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelvgg.layers.pop(18)\n",
    "modelvgg.layers.pop(17)\n",
    "modelvgg.layers.pop(16)\n",
    "modelvgg.layers.pop(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#modelvgg.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features = modelvgg.predict(new_X_train)\n",
    "print('train_features shape and type',train_features.shape,train_features.dtype)\n",
    "validation_features = modelvgg.predict(new_X_val)\n",
    "print('validation_features shape and type',validation_features.shape,train_features.dtype)\n",
    "test_features = modelvgg.predict(X_test)\n",
    "print('test_features shape and type',test_features.shape,train_features.dtype)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_features.shape[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_name = '../cifar_redux_augmented_vgg'\n",
    "\n",
    "modelVGG = Sequential()\n",
    "modelVGG.add(Flatten(input_shape= train_features.shape[1:]))    \n",
    "modelVGG.add(Dense(120))\n",
    "modelVGG.add(Activation('relu'))\n",
    "modelVGG.add(Dropout(0.5))\n",
    "modelVGG.add(Dense(3))\n",
    "modelVGG.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "modelVGG.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treinando class MyCb(TrainingPlotter):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class MyCb(TrainingPlotter):\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        super().on_epoch_end(epoch, logs)\n",
    "\n",
    "\n",
    "def train_network(model, model_name, Xtra, ytra, Xval, yval, \n",
    "                  opt='rmsprop', batch_size=100, nepochs=50, patience=50, reset=False, ploss=1.0):\n",
    "\n",
    "    do_plot = (ploss > 0.0)\n",
    "    \n",
    "    model_fn = model_name + '.model'\n",
    "    if reset and os.path.isfile(model_fn):\n",
    "        os.unlink(model_name + '.model')\n",
    "        \n",
    "    if not os.path.isfile(model_fn):\n",
    "        # initialize the optimizer and model\n",
    "        print(\"[INFO] compiling model...\")\n",
    "        model.compile(loss=\"categorical_crossentropy\", optimizer=opt, metrics=[\"accuracy\"])    \n",
    "\n",
    "        # History, checkpoint, earlystop, plot losses:\n",
    "        cb = MyCb(n=1, filepath=model_name, patience=patience, plot_losses=do_plot)\n",
    "        \n",
    "    else:\n",
    "        print(\"[INFO] loading model...\")\n",
    "        model, cb = load_model_and_history(model_name)\n",
    "        cb.patience = patience\n",
    "\n",
    "    past_epochs = cb.get_nepochs()\n",
    "    tr_epochs = nepochs - past_epochs\n",
    "    \n",
    "    if do_plot:\n",
    "        vv = 0\n",
    "        fig = plot.figure(figsize=(15,6))\n",
    "        plot.ylim(0.0, ploss)\n",
    "        plot.xlim(0, nepochs)\n",
    "        plot.grid(True)\n",
    "    else:\n",
    "        vv = 2\n",
    "\n",
    "    print(\"[INFO] training for {} epochs ...\".format(tr_epochs))\n",
    "    try:\n",
    "         model.fit(Xtra, ytra, batch_size=batch_size, epochs=tr_epochs, verbose=vv, \n",
    "                      validation_data=(Xval,yval), callbacks=[cb])\n",
    "    except KeyboardInterrupt:\n",
    "        pass\n",
    "\n",
    "    model, histo = load_model_and_history(model_name)\n",
    "    return model, cb\n",
    "\n",
    "\n",
    "def test_network(model_name, Xtest, ytest, batch_size=40):\n",
    "    model, histo = load_model_and_history(model_name)\n",
    "    print('Model from epoch {}'.format(histo.best_epoch))\n",
    "    print(\"[INFO] evaluating in the test data set ...\")\n",
    "    loss, accuracy = model.evaluate(Xtest, ytest, batch_size=batch_size, verbose=1)\n",
    "    print(\"\\n[INFO] accuracy on the test data set: {:.2f}% [{:.5f}]\".format(accuracy * 100, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('train_features.shape',train_features.shape)\n",
    "print('validation_features.shape',validation_features.shape)\n",
    "print('test_features.shape',test_features.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fit_params = {\n",
    "    'opt':         'adam',           # SGD(lr=0.01, momentum=0.9, nesterov=True), \n",
    "    'nepochs':         100, \n",
    "    'patience':        30,\n",
    "    'ploss':           1.5,\n",
    "    'reset':           True,\n",
    "}\n",
    "\n",
    "train_network(modelVGG, model_name, train_features, y_train_oh, validation_features, y_val_oh,  **fit_params);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_network(model_name, test_features,y_test_oh,X_test.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.applications.vgg16 import VGG16\n",
    "\n",
    "print(\"[INFO] creating model...\")\n",
    "#vgg = VGG16(include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3))\n",
    "vgg = VGG16(include_top=False, weights='imagenet')\n",
    "vgg.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Construção da rede neural"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(train_features.shape)\n",
    "print(new_X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "img_height, img_width = new_X_train.shape[2],new_X_train.shape[3]\n",
    "print(img_height,img_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!ls .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "\n",
    "model_name = '../cifar10_vgg_finetune'   # modelo da rede atual\n",
    "top_model_name = '../cifar_redux_augmented_vgg'\n",
    "nb_classes=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Model(inputs=vgg.input, outputs=x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_net(top_model_name):\n",
    "    from keras.applications.vgg16 import VGG16\n",
    "    \n",
    "    print(\"[INFO] creating model...\")\n",
    "    #vgg = VGG16(include_top=False, weights='imagenet', input_shape=(img_height, img_width, 3))\n",
    "    #vgg = VGG16(include_top=False, weights='imagenet', input_shape=(3,img_height, img_width))\n",
    "    vgg = VGG16(include_top=False, weights='imagenet', classes=nb_classes, pooling='max')\n",
    "    \n",
    "    print(vgg.output)\n",
    "    # build a classifier model and put on top of the convolutional model\n",
    "    #x = Flatten()(vgg.output)\n",
    "    x = Dense(120, activation='relu', name='dense1')(vgg.output)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(3, activation='relu', name='d1')(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = Dense(1, activation='sigmoid', name='d2')(x)\n",
    "    \n",
    "    #x = Dense(40, activation='relu', name='dense1')(vgg.output)\n",
    "   # x = Dropout(0.5)(x)\n",
    "    #x = Dense(120, activation='relu', name='dense2')(x)\n",
    "    #x = Dropout(0.2)(x)\n",
    "    #x = Dense(nb_classes, activation='softmax', name='dense3')(x)\n",
    "\n",
    "    #model = Model(inputs=vgg.input, outputs=x\n",
    "    \n",
    "    \n",
    "    model = Model(inputs=vgg.input, outputs=x)\n",
    "    print(model.layers)\n",
    "    print(len(model.layers))    #     print('Model layers:')\n",
    "    #     for i, layer in enumerate(model.layers):\n",
    "    #         print('    {:2d} {:15s} {}'.format(i, layer.name, layer))\n",
    "    \n",
    "    # modelo da rede densa treinada no notebook anterior\n",
    "    top_model_name = top_model_name\n",
    "    # Carrego os pesos treinados anteriormente\n",
    "    #w1, b1, w2, b2 = load_model(top_model_name).get_weights()    \n",
    "    w1, b1, w2, b2 = modelVGG.get_weights()    \n",
    "    print(w1.shape,b1.shape,w2.shape,b2.shape)\n",
    "    # Coloco nas camadas densas finais da rede\n",
    "    model.layers[20].set_weights([w1, b1])\n",
    "    model.layers[22].set_weights([w2, b2])\n",
    "    \n",
    "    # Torno não-treináveis as primeiras 15 camadas\n",
    "    # da rede (os pesos não serão alterados)\n",
    "    for layer in model.layers[:15]:\n",
    "        layer.trainable = False\n",
    "        \n",
    "    return model\n",
    "\n",
    "model = build_net(top_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "batch_size = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(new_X_train.shape, y_train_oh.shape,new_X_val.shape, y_val_oh.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(1600, 32, 32, 3) (1600, 3) (400, 32, 32, 3) (400, 3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = model.fit(new_X_train.reshape(1700,3,32,32), y_train_oh,\n",
    "              validation_data=(new_X_val.reshape(300,3,32,32), y_val_oh),\n",
    "              batch_size=batch_size,\n",
    "              epochs=100,\n",
    "              )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "h = model.fit(X_train[train_i], y_train_oh[train_i],\n",
    "              validation_data=(X_train[val_i], y_train_oh[val_i]),\n",
    "              batch_size=batch_size,\n",
    "              epochs=400,\n",
    "              callbacks=[early_stopping, checkpointer, reduce_lr], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model_name = '../cifar10_vgg_finetune'\n",
    "fit_params = {\n",
    "    'opt':         'adam',           # SGD(lr=0.01, momentum=0.9, nesterov=True), \n",
    "    'nepochs':         100, \n",
    "    'patience':        30,\n",
    "    'ploss':           1.5,\n",
    "    'reset':           True,\n",
    "}\n",
    "\n",
    "train_network(model, model_name, train_features, y_train_oh, validation_features, y_val_oh,  **fit_params);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
