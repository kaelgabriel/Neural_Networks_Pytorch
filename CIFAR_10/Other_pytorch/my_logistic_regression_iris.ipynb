{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Meu nome é: Gabriel Moraes Barros \n",
      "Meu RA é: 192801\n"
     ]
    }
   ],
   "source": [
    "print('Meu nome é: Gabriel Moraes Barros ')\n",
    "print('Meu RA é: 192801')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.utils import np_utils "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEKCAYAAAD9xUlFAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\neAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f49f061ada0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "iris = load_iris()\n",
    "X = iris.data[:,::2]  # comprimento das sépalas e pétalas, indices 0 e 2\n",
    "Xc = X - X.min(axis=0)\n",
    "Xc /= Xc.max(axis=0)\n",
    "\n",
    "Y = iris.target\n",
    "\n",
    "#X[:,0] = Sepal lenght\n",
    "#X[:,1] = Sepal width\n",
    "#define  colors to be red, yellow and blue\n",
    "colors = np.array(['r','y','b'])\n",
    "plt.scatter(Xc[:, 0], Xc[:, 1], c=colors[Y])\n",
    "plt.xlabel('Sepal length')\n",
    "plt.ylabel('Sepal width')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(150, 2)\n",
      "(150,)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)\n",
    "print(Y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0]\n",
      "[[ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]\n",
      " [ 1.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "Y_oh = np_utils.to_categorical(Y, 3) \n",
    "print(Y[0:5])\n",
    "print(Y_oh[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "150\n"
     ]
    }
   ],
   "source": [
    "print(X.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelo do Torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "n_classes=3\n",
    "# N is batch size; D_in is input dimension;\n",
    "# H is hidden dimension; D_out is output dimension.\n",
    "N, D_in, H, D_out = 10, X.shape[1], 10, n_classes "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_torch = torch.nn.Sequential(\n",
    "    torch.nn.Linear(X.shape[1], 2*H),\n",
    "    torch.nn.ReLU(),\n",
    "    \n",
    "    torch.nn.Linear(2*H, H),\n",
    "    torch.nn.ReLU(),\n",
    "\n",
    "    torch.nn.Linear(H, n_classes),\n",
    "    torch.nn.Softmax()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "learning_rate = 1e-2\n",
    "optimizer = torch.optim.Adam(model_torch.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "opt = optim.SGD(model_torch.parameters(), lr=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([150, 2])\n",
      "torch.Size([150])\n"
     ]
    }
   ],
   "source": [
    "#X_tensor = torch.from_numpy(Xc).type(torch.FloatTensor)\n",
    "X_tensor = Variable(torch.from_numpy(Xc).type(torch.FloatTensor))\n",
    "\n",
    "print(X_tensor.size())\n",
    "#Y_tensor = torch.from_numpy(Y).type(torch.LongTensor)\n",
    "Y_tensor = Variable(torch.from_numpy(Y).type(torch.LongTensor), requires_grad=False)\n",
    "print(Y_tensor.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration , Loss 0 1.1034729480743408\n",
      "Iteration , Loss 1 1.1014769077301025\n",
      "Iteration , Loss 2 1.0998504161834717\n",
      "Iteration , Loss 3 1.0984543561935425\n",
      "Iteration , Loss 4 1.0972102880477905\n",
      "Iteration , Loss 5 1.0963257551193237\n",
      "Iteration , Loss 6 1.0956257581710815\n",
      "Iteration , Loss 7 1.0948216915130615\n",
      "Iteration , Loss 8 1.0938688516616821\n",
      "Iteration , Loss 9 1.0927573442459106\n",
      "Iteration , Loss 10 1.091389536857605\n",
      "Iteration , Loss 11 1.0895615816116333\n",
      "Iteration , Loss 12 1.0873868465423584\n",
      "Iteration , Loss 13 1.0849816799163818\n",
      "Iteration , Loss 14 1.0822240114212036\n",
      "Iteration , Loss 15 1.0791062116622925\n",
      "Iteration , Loss 16 1.075727939605713\n",
      "Iteration , Loss 17 1.0721665620803833\n",
      "Iteration , Loss 18 1.0682121515274048\n",
      "Iteration , Loss 19 1.0638071298599243\n",
      "Iteration , Loss 20 1.0588854551315308\n",
      "Iteration , Loss 21 1.053410291671753\n",
      "Iteration , Loss 22 1.04734206199646\n",
      "Iteration , Loss 23 1.0407015085220337\n",
      "Iteration , Loss 24 1.0334968566894531\n",
      "Iteration , Loss 25 1.0256868600845337\n",
      "Iteration , Loss 26 1.0172582864761353\n",
      "Iteration , Loss 27 1.0082629919052124\n",
      "Iteration , Loss 28 0.9987635016441345\n",
      "Iteration , Loss 29 0.988828718662262\n",
      "Iteration , Loss 30 0.9785471558570862\n",
      "Iteration , Loss 31 0.9680031538009644\n",
      "Iteration , Loss 32 0.9572960138320923\n",
      "Iteration , Loss 33 0.9465667605400085\n",
      "Iteration , Loss 34 0.9359632134437561\n",
      "Iteration , Loss 35 0.9256114959716797\n",
      "Iteration , Loss 36 0.9155813455581665\n",
      "Iteration , Loss 37 0.905834972858429\n",
      "Iteration , Loss 38 0.8963502049446106\n",
      "Iteration , Loss 39 0.8871133327484131\n",
      "Iteration , Loss 40 0.8781275153160095\n",
      "Iteration , Loss 41 0.869410514831543\n",
      "Iteration , Loss 42 0.8609790802001953\n",
      "Iteration , Loss 43 0.8528689742088318\n",
      "Iteration , Loss 44 0.8451123237609863\n",
      "Iteration , Loss 45 0.8377836346626282\n",
      "Iteration , Loss 46 0.8309119939804077\n",
      "Iteration , Loss 47 0.8245166540145874\n",
      "Iteration , Loss 48 0.8185500502586365\n",
      "Iteration , Loss 49 0.8128979802131653\n",
      "Iteration , Loss 50 0.8073473572731018\n",
      "Iteration , Loss 51 0.8017812967300415\n",
      "Iteration , Loss 52 0.7960431575775146\n",
      "Iteration , Loss 53 0.7899986505508423\n",
      "Iteration , Loss 54 0.7836043834686279\n",
      "Iteration , Loss 55 0.7768520712852478\n",
      "Iteration , Loss 56 0.7697290778160095\n",
      "Iteration , Loss 57 0.7623573541641235\n",
      "Iteration , Loss 58 0.754980742931366\n",
      "Iteration , Loss 59 0.7475924491882324\n",
      "Iteration , Loss 60 0.7405703067779541\n",
      "Iteration , Loss 61 0.7339409589767456\n",
      "Iteration , Loss 62 0.7280281782150269\n",
      "Iteration , Loss 63 0.7227717638015747\n",
      "Iteration , Loss 64 0.7179005146026611\n",
      "Iteration , Loss 65 0.7132613658905029\n",
      "Iteration , Loss 66 0.7088222503662109\n",
      "Iteration , Loss 67 0.7046002149581909\n",
      "Iteration , Loss 68 0.7005642652511597\n",
      "Iteration , Loss 69 0.6966579556465149\n",
      "Iteration , Loss 70 0.6928955316543579\n",
      "Iteration , Loss 71 0.6893746256828308\n",
      "Iteration , Loss 72 0.6860175132751465\n",
      "Iteration , Loss 73 0.6827318072319031\n",
      "Iteration , Loss 74 0.6795510649681091\n",
      "Iteration , Loss 75 0.6765292882919312\n",
      "Iteration , Loss 76 0.6736226677894592\n",
      "Iteration , Loss 77 0.6707893013954163\n",
      "Iteration , Loss 78 0.668083667755127\n",
      "Iteration , Loss 79 0.6655048131942749\n",
      "Iteration , Loss 80 0.6629887223243713\n",
      "Iteration , Loss 81 0.6605403423309326\n",
      "Iteration , Loss 82 0.6581889390945435\n",
      "Iteration , Loss 83 0.6558914184570312\n",
      "Iteration , Loss 84 0.653637170791626\n",
      "Iteration , Loss 85 0.6514602303504944\n",
      "Iteration , Loss 86 0.6493417024612427\n",
      "Iteration , Loss 87 0.6472570300102234\n",
      "Iteration , Loss 88 0.6452343463897705\n",
      "Iteration , Loss 89 0.6432628631591797\n",
      "Iteration , Loss 90 0.6413179039955139\n",
      "Iteration , Loss 91 0.6394234299659729\n",
      "Iteration , Loss 92 0.6375758051872253\n",
      "Iteration , Loss 93 0.635759711265564\n",
      "Iteration , Loss 94 0.633995771408081\n",
      "Iteration , Loss 95 0.632276713848114\n",
      "Iteration , Loss 96 0.6305939555168152\n",
      "Iteration , Loss 97 0.6289642453193665\n",
      "Iteration , Loss 98 0.6273780465126038\n",
      "Iteration , Loss 99 0.625835120677948\n",
      "Iteration , Loss 100 0.6243441700935364\n",
      "Iteration , Loss 101 0.6228941679000854\n",
      "Iteration , Loss 102 0.6214925646781921\n",
      "Iteration , Loss 103 0.6201406121253967\n",
      "Iteration , Loss 104 0.6188330054283142\n",
      "Iteration , Loss 105 0.6175774335861206\n",
      "Iteration , Loss 106 0.616367518901825\n",
      "Iteration , Loss 107 0.6152044534683228\n",
      "Iteration , Loss 108 0.6140910387039185\n",
      "Iteration , Loss 109 0.6130217909812927\n",
      "Iteration , Loss 110 0.6120020151138306\n",
      "Iteration , Loss 111 0.6110265254974365\n",
      "Iteration , Loss 112 0.6100967526435852\n",
      "Iteration , Loss 113 0.6092133522033691\n",
      "Iteration , Loss 114 0.6083725690841675\n",
      "Iteration , Loss 115 0.6075765490531921\n",
      "Iteration , Loss 116 0.6068217158317566\n",
      "Iteration , Loss 117 0.6061089634895325\n",
      "Iteration , Loss 118 0.6054354906082153\n",
      "Iteration , Loss 119 0.6047989130020142\n",
      "Iteration , Loss 120 0.6041990518569946\n",
      "Iteration , Loss 121 0.6036314368247986\n",
      "Iteration , Loss 122 0.6030953526496887\n",
      "Iteration , Loss 123 0.6025878190994263\n",
      "Iteration , Loss 124 0.6021063923835754\n",
      "Iteration , Loss 125 0.6016478538513184\n",
      "Iteration , Loss 126 0.6012102365493774\n",
      "Iteration , Loss 127 0.6007912158966064\n",
      "Iteration , Loss 128 0.6003889441490173\n",
      "Iteration , Loss 129 0.6000015735626221\n",
      "Iteration , Loss 130 0.5996263027191162\n",
      "Iteration , Loss 131 0.5992631316184998\n",
      "Iteration , Loss 132 0.5989099144935608\n",
      "Iteration , Loss 133 0.5985656976699829\n",
      "Iteration , Loss 134 0.5982297658920288\n",
      "Iteration , Loss 135 0.5979021787643433\n",
      "Iteration , Loss 136 0.5975820422172546\n",
      "Iteration , Loss 137 0.5972692370414734\n",
      "Iteration , Loss 138 0.59696364402771\n",
      "Iteration , Loss 139 0.5966651439666748\n",
      "Iteration , Loss 140 0.5963744521141052\n",
      "Iteration , Loss 141 0.5960908532142639\n",
      "Iteration , Loss 142 0.5958149433135986\n",
      "Iteration , Loss 143 0.5955459475517273\n",
      "Iteration , Loss 144 0.5952847003936768\n",
      "Iteration , Loss 145 0.5950305461883545\n",
      "Iteration , Loss 146 0.5947834849357605\n",
      "Iteration , Loss 147 0.5945437550544739\n",
      "Iteration , Loss 148 0.5943108797073364\n",
      "Iteration , Loss 149 0.5940848588943481\n",
      "Iteration , Loss 150 0.5938660502433777\n",
      "Iteration , Loss 151 0.5936532616615295\n",
      "Iteration , Loss 152 0.5934463739395142\n",
      "Iteration , Loss 153 0.5932453870773315\n",
      "Iteration , Loss 154 0.5930496454238892\n",
      "Iteration , Loss 155 0.5928588509559631\n",
      "Iteration , Loss 156 0.5926728248596191\n",
      "Iteration , Loss 157 0.592491626739502\n",
      "Iteration , Loss 158 0.5923143625259399\n",
      "Iteration , Loss 159 0.592141330242157\n",
      "Iteration , Loss 160 0.591972291469574\n",
      "Iteration , Loss 161 0.5918065905570984\n",
      "Iteration , Loss 162 0.5916444063186646\n",
      "Iteration , Loss 163 0.5914853811264038\n",
      "Iteration , Loss 164 0.5913300514221191\n",
      "Iteration , Loss 165 0.5911774635314941\n",
      "Iteration , Loss 166 0.5910282135009766\n",
      "Iteration , Loss 167 0.5908815860748291\n",
      "Iteration , Loss 168 0.5907385349273682\n",
      "Iteration , Loss 169 0.5905975103378296\n",
      "Iteration , Loss 170 0.590459942817688\n",
      "Iteration , Loss 171 0.5903245806694031\n",
      "Iteration , Loss 172 0.590192437171936\n",
      "Iteration , Loss 173 0.5900624990463257\n",
      "Iteration , Loss 174 0.589935302734375\n",
      "Iteration , Loss 175 0.5898109078407288\n",
      "Iteration , Loss 176 0.5896885395050049\n",
      "Iteration , Loss 177 0.5895688533782959\n",
      "Iteration , Loss 178 0.5894513130187988\n",
      "Iteration , Loss 179 0.5893361568450928\n",
      "Iteration , Loss 180 0.5892231464385986\n",
      "Iteration , Loss 181 0.5891119837760925\n",
      "Iteration , Loss 182 0.5890030264854431\n",
      "Iteration , Loss 183 0.5888957977294922\n",
      "Iteration , Loss 184 0.5887906551361084\n",
      "Iteration , Loss 185 0.5886877179145813\n",
      "Iteration , Loss 186 0.5885862112045288\n",
      "Iteration , Loss 187 0.588486909866333\n",
      "Iteration , Loss 188 0.5883888006210327\n",
      "Iteration , Loss 189 0.5882925391197205\n",
      "Iteration , Loss 190 0.5881979465484619\n",
      "Iteration , Loss 191 0.5881050229072571\n",
      "Iteration , Loss 192 0.5880134105682373\n",
      "Iteration , Loss 193 0.5879234075546265\n",
      "Iteration , Loss 194 0.5878348350524902\n",
      "Iteration , Loss 195 0.5877476334571838\n",
      "Iteration , Loss 196 0.587661862373352\n",
      "Iteration , Loss 197 0.5875775218009949\n",
      "Iteration , Loss 198 0.5874946117401123\n",
      "Iteration , Loss 199 0.5874127745628357\n",
      "Iteration , Loss 200 0.587332546710968\n",
      "Iteration , Loss 201 0.5872533917427063\n",
      "Iteration , Loss 202 0.5871753692626953\n",
      "Iteration , Loss 203 0.5870987772941589\n",
      "Iteration , Loss 204 0.5870233774185181\n",
      "Iteration , Loss 205 0.5869491100311279\n",
      "Iteration , Loss 206 0.5868759155273438\n",
      "Iteration , Loss 207 0.5868040919303894\n",
      "Iteration , Loss 208 0.5867332816123962\n",
      "Iteration , Loss 209 0.5866634249687195\n",
      "Iteration , Loss 210 0.5865947604179382\n",
      "Iteration , Loss 211 0.5865267515182495\n",
      "Iteration , Loss 212 0.5864599347114563\n",
      "Iteration , Loss 213 0.5863943696022034\n",
      "Iteration , Loss 214 0.5863292813301086\n",
      "Iteration , Loss 215 0.5862652063369751\n",
      "Iteration , Loss 216 0.5862022638320923\n",
      "Iteration , Loss 217 0.5861403346061707\n",
      "Iteration , Loss 218 0.5860790610313416\n",
      "Iteration , Loss 219 0.5860186815261841\n",
      "Iteration , Loss 220 0.5859591364860535\n",
      "Iteration , Loss 221 0.5859005451202393\n",
      "Iteration , Loss 222 0.5858427286148071\n",
      "Iteration , Loss 223 0.5857858061790466\n",
      "Iteration , Loss 224 0.5857298374176025\n",
      "Iteration , Loss 225 0.5856744647026062\n",
      "Iteration , Loss 226 0.5856200456619263\n",
      "Iteration , Loss 227 0.5855661034584045\n",
      "Iteration , Loss 228 0.5855132341384888\n",
      "Iteration , Loss 229 0.5854611396789551\n",
      "Iteration , Loss 230 0.5854096412658691\n",
      "Iteration , Loss 231 0.5853588581085205\n",
      "Iteration , Loss 232 0.5853086113929749\n",
      "Iteration , Loss 233 0.5852591395378113\n",
      "Iteration , Loss 234 0.5852102041244507\n",
      "Iteration , Loss 235 0.5851620435714722\n",
      "Iteration , Loss 236 0.5851143598556519\n",
      "Iteration , Loss 237 0.585067629814148\n",
      "Iteration , Loss 238 0.5850210785865784\n",
      "Iteration , Loss 239 0.5849753022193909\n",
      "Iteration , Loss 240 0.584930419921875\n",
      "Iteration , Loss 241 0.5848856568336487\n",
      "Iteration , Loss 242 0.5848417282104492\n",
      "Iteration , Loss 243 0.584798276424408\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration , Loss 244 0.5847553610801697\n",
      "Iteration , Loss 245 0.5847131609916687\n",
      "Iteration , Loss 246 0.5846713185310364\n",
      "Iteration , Loss 247 0.5846301317214966\n",
      "Iteration , Loss 248 0.5845891237258911\n",
      "Iteration , Loss 249 0.5845488905906677\n",
      "Iteration , Loss 250 0.5845093131065369\n",
      "Iteration , Loss 251 0.5844700336456299\n",
      "Iteration , Loss 252 0.5844310522079468\n",
      "Iteration , Loss 253 0.5843926072120667\n",
      "Iteration , Loss 254 0.5843550562858582\n",
      "Iteration , Loss 255 0.584317684173584\n",
      "Iteration , Loss 256 0.584280788898468\n",
      "Iteration , Loss 257 0.584244430065155\n",
      "Iteration , Loss 258 0.5842084884643555\n",
      "Iteration , Loss 259 0.5841727256774902\n",
      "Iteration , Loss 260 0.584137499332428\n",
      "Iteration , Loss 261 0.5841028690338135\n",
      "Iteration , Loss 262 0.5840684175491333\n",
      "Iteration , Loss 263 0.5840345025062561\n",
      "Iteration , Loss 264 0.5840007662773132\n",
      "Iteration , Loss 265 0.5839678645133972\n",
      "Iteration , Loss 266 0.5839349031448364\n",
      "Iteration , Loss 267 0.5839027166366577\n",
      "Iteration , Loss 268 0.5838707089424133\n",
      "Iteration , Loss 269 0.583838939666748\n",
      "Iteration , Loss 270 0.583807647228241\n",
      "Iteration , Loss 271 0.5837767124176025\n",
      "Iteration , Loss 272 0.5837463140487671\n",
      "Iteration , Loss 273 0.5837161540985107\n",
      "Iteration , Loss 274 0.5836861729621887\n",
      "Iteration , Loss 275 0.5836568474769592\n",
      "Iteration , Loss 276 0.583627462387085\n",
      "Iteration , Loss 277 0.5835985541343689\n",
      "Iteration , Loss 278 0.5835701823234558\n",
      "Iteration , Loss 279 0.583541989326477\n",
      "Iteration , Loss 280 0.5835141539573669\n",
      "Iteration , Loss 281 0.5834864377975464\n",
      "Iteration , Loss 282 0.5834592580795288\n",
      "Iteration , Loss 283 0.5834324955940247\n",
      "Iteration , Loss 284 0.5834057927131653\n",
      "Iteration , Loss 285 0.5833793878555298\n",
      "Iteration , Loss 286 0.5833534598350525\n",
      "Iteration , Loss 287 0.58332759141922\n",
      "Iteration , Loss 288 0.5833021402359009\n",
      "Iteration , Loss 289 0.5832770466804504\n",
      "Iteration , Loss 290 0.5832520127296448\n",
      "Iteration , Loss 291 0.5832274556159973\n",
      "Iteration , Loss 292 0.5832028985023499\n",
      "Iteration , Loss 293 0.5831789374351501\n",
      "Iteration , Loss 294 0.5831550359725952\n",
      "Iteration , Loss 295 0.5831312537193298\n",
      "Iteration , Loss 296 0.5831078886985779\n",
      "Iteration , Loss 297 0.5830847024917603\n",
      "Iteration , Loss 298 0.583061933517456\n",
      "Iteration , Loss 299 0.5830393433570862\n",
      "Iteration , Loss 300 0.5830169916152954\n",
      "Iteration , Loss 301 0.5829947590827942\n",
      "Iteration , Loss 302 0.5829727053642273\n",
      "Iteration , Loss 303 0.5829510688781738\n",
      "Iteration , Loss 304 0.5829295516014099\n",
      "Iteration , Loss 305 0.5829084515571594\n",
      "Iteration , Loss 306 0.5828873515129089\n",
      "Iteration , Loss 307 0.5828666687011719\n",
      "Iteration , Loss 308 0.5828459858894348\n",
      "Iteration , Loss 309 0.5828255414962769\n",
      "Iteration , Loss 310 0.582805335521698\n",
      "Iteration , Loss 311 0.5827856063842773\n",
      "Iteration , Loss 312 0.5827656984329224\n",
      "Iteration , Loss 313 0.5827459692955017\n",
      "Iteration , Loss 314 0.582726776599884\n",
      "Iteration , Loss 315 0.5827073454856873\n",
      "Iteration , Loss 316 0.5826886296272278\n",
      "Iteration , Loss 317 0.5826694965362549\n",
      "Iteration , Loss 318 0.5826511383056641\n",
      "Iteration , Loss 319 0.5826327800750732\n",
      "Iteration , Loss 320 0.582614541053772\n",
      "Iteration , Loss 321 0.5825963616371155\n",
      "Iteration , Loss 322 0.5825784802436829\n",
      "Iteration , Loss 323 0.5825608968734741\n",
      "Iteration , Loss 324 0.5825432538986206\n",
      "Iteration , Loss 325 0.5825260877609253\n",
      "Iteration , Loss 326 0.5825086832046509\n",
      "Iteration , Loss 327 0.5824918150901794\n",
      "Iteration , Loss 328 0.582474946975708\n",
      "Iteration , Loss 329 0.5824583172798157\n",
      "Iteration , Loss 330 0.5824416875839233\n",
      "Iteration , Loss 331 0.5824254155158997\n",
      "Iteration , Loss 332 0.5824092030525208\n",
      "Iteration , Loss 333 0.5823929905891418\n",
      "Iteration , Loss 334 0.5823772549629211\n",
      "Iteration , Loss 335 0.5823614597320557\n",
      "Iteration , Loss 336 0.5823461413383484\n",
      "Iteration , Loss 337 0.5823306441307068\n",
      "Iteration , Loss 338 0.5823150873184204\n",
      "Iteration , Loss 339 0.5823000073432922\n",
      "Iteration , Loss 340 0.5822850465774536\n",
      "Iteration , Loss 341 0.5822702646255493\n",
      "Iteration , Loss 342 0.5822556614875793\n",
      "Iteration , Loss 343 0.582240879535675\n",
      "Iteration , Loss 344 0.5822263360023499\n",
      "Iteration , Loss 345 0.582211971282959\n",
      "Iteration , Loss 346 0.582197904586792\n",
      "Iteration , Loss 347 0.5821839570999146\n",
      "Iteration , Loss 348 0.5821700692176819\n",
      "Iteration , Loss 349 0.5821563601493835\n",
      "Iteration , Loss 350 0.58214271068573\n",
      "Iteration , Loss 351 0.5821291208267212\n",
      "Iteration , Loss 352 0.5821155905723572\n",
      "Iteration , Loss 353 0.5821022987365723\n",
      "Iteration , Loss 354 0.5820891261100769\n",
      "Iteration , Loss 355 0.5820761322975159\n",
      "Iteration , Loss 356 0.5820631384849548\n",
      "Iteration , Loss 357 0.5820503234863281\n",
      "Iteration , Loss 358 0.582037627696991\n",
      "Iteration , Loss 359 0.5820251703262329\n",
      "Iteration , Loss 360 0.5820125341415405\n",
      "Iteration , Loss 361 0.582000195980072\n",
      "Iteration , Loss 362 0.5819878578186035\n",
      "Iteration , Loss 363 0.5819758176803589\n",
      "Iteration , Loss 364 0.581963837146759\n",
      "Iteration , Loss 365 0.5819520354270935\n",
      "Iteration , Loss 366 0.5819400548934937\n",
      "Iteration , Loss 367 0.5819284319877625\n",
      "Iteration , Loss 368 0.5819168090820312\n",
      "Iteration , Loss 369 0.5819053053855896\n",
      "Iteration , Loss 370 0.5818937420845032\n",
      "Iteration , Loss 371 0.5818823575973511\n",
      "Iteration , Loss 372 0.5818712711334229\n",
      "Iteration , Loss 373 0.5818603038787842\n",
      "Iteration , Loss 374 0.5818490386009216\n",
      "Iteration , Loss 375 0.5818382501602173\n",
      "Iteration , Loss 376 0.5818272233009338\n",
      "Iteration , Loss 377 0.5818166136741638\n",
      "Iteration , Loss 378 0.5818060040473938\n",
      "Iteration , Loss 379 0.5817954540252686\n",
      "Iteration , Loss 380 0.5817850828170776\n",
      "Iteration , Loss 381 0.5817747116088867\n",
      "Iteration , Loss 382 0.5817646384239197\n",
      "Iteration , Loss 383 0.5817542672157288\n",
      "Iteration , Loss 384 0.5817441344261169\n",
      "Iteration , Loss 385 0.5817340612411499\n",
      "Iteration , Loss 386 0.5817241072654724\n",
      "Iteration , Loss 387 0.5817143321037292\n",
      "Iteration , Loss 388 0.5817044377326965\n",
      "Iteration , Loss 389 0.5816948413848877\n",
      "Iteration , Loss 390 0.5816851854324341\n",
      "Iteration , Loss 391 0.5816758275032043\n",
      "Iteration , Loss 392 0.5816664099693298\n",
      "Iteration , Loss 393 0.5816570520401001\n",
      "Iteration , Loss 394 0.5816476941108704\n",
      "Iteration , Loss 395 0.581638514995575\n",
      "Iteration , Loss 396 0.5816295742988586\n",
      "Iteration , Loss 397 0.5816203355789185\n",
      "Iteration , Loss 398 0.5816113948822021\n",
      "Iteration , Loss 399 0.5816023945808411\n",
      "Iteration , Loss 400 0.5815936923027039\n",
      "Iteration , Loss 401 0.5815847516059875\n",
      "Iteration , Loss 402 0.5815762281417847\n",
      "Iteration , Loss 403 0.5815674066543579\n",
      "Iteration , Loss 404 0.581558883190155\n",
      "Iteration , Loss 405 0.5815503001213074\n",
      "Iteration , Loss 406 0.5815420746803284\n",
      "Iteration , Loss 407 0.5815337300300598\n",
      "Iteration , Loss 408 0.5815255045890808\n",
      "Iteration , Loss 409 0.5815171599388123\n",
      "Iteration , Loss 410 0.5815090537071228\n",
      "Iteration , Loss 411 0.5815010070800781\n",
      "Iteration , Loss 412 0.581493079662323\n",
      "Iteration , Loss 413 0.5814850330352783\n",
      "Iteration , Loss 414 0.5814770460128784\n",
      "Iteration , Loss 415 0.5814691781997681\n",
      "Iteration , Loss 416 0.5814613699913025\n",
      "Iteration , Loss 417 0.5814538598060608\n",
      "Iteration , Loss 418 0.5814459323883057\n",
      "Iteration , Loss 419 0.5814383029937744\n",
      "Iteration , Loss 420 0.5814307928085327\n",
      "Iteration , Loss 421 0.5814234018325806\n",
      "Iteration , Loss 422 0.581416130065918\n",
      "Iteration , Loss 423 0.5814085006713867\n",
      "Iteration , Loss 424 0.5814012885093689\n",
      "Iteration , Loss 425 0.5813942551612854\n",
      "Iteration , Loss 426 0.581386923789978\n",
      "Iteration , Loss 427 0.581379771232605\n",
      "Iteration , Loss 428 0.5813727974891663\n",
      "Iteration , Loss 429 0.5813655257225037\n",
      "Iteration , Loss 430 0.5813587307929993\n",
      "Iteration , Loss 431 0.5813516974449158\n",
      "Iteration , Loss 432 0.5813450217247009\n",
      "Iteration , Loss 433 0.5813381671905518\n",
      "Iteration , Loss 434 0.5813314914703369\n",
      "Iteration , Loss 435 0.5813245177268982\n",
      "Iteration , Loss 436 0.5813180804252625\n",
      "Iteration , Loss 437 0.5813113451004028\n",
      "Iteration , Loss 438 0.5813049077987671\n",
      "Iteration , Loss 439 0.581298291683197\n",
      "Iteration , Loss 440 0.5812917351722717\n",
      "Iteration , Loss 441 0.5812854170799255\n",
      "Iteration , Loss 442 0.581278920173645\n",
      "Iteration , Loss 443 0.5812727212905884\n",
      "Iteration , Loss 444 0.5812664031982422\n",
      "Iteration , Loss 445 0.5812602639198303\n",
      "Iteration , Loss 446 0.5812540650367737\n",
      "Iteration , Loss 447 0.581247866153717\n",
      "Iteration , Loss 448 0.58124178647995\n",
      "Iteration , Loss 449 0.5812358856201172\n",
      "Iteration , Loss 450 0.5812297463417053\n",
      "Iteration , Loss 451 0.5812237858772278\n",
      "Iteration , Loss 452 0.5812180042266846\n",
      "Iteration , Loss 453 0.581212043762207\n",
      "Iteration , Loss 454 0.5812064409255981\n",
      "Iteration , Loss 455 0.5812005400657654\n",
      "Iteration , Loss 456 0.5811948180198669\n",
      "Iteration , Loss 457 0.581188976764679\n",
      "Iteration , Loss 458 0.5811835527420044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration , Loss 459 0.5811777710914612\n",
      "Iteration , Loss 460 0.5811723470687866\n",
      "Iteration , Loss 461 0.5811667442321777\n",
      "Iteration , Loss 462 0.5811613202095032\n",
      "Iteration , Loss 463 0.5811557769775391\n",
      "Iteration , Loss 464 0.5811504125595093\n",
      "Iteration , Loss 465 0.5811450481414795\n",
      "Iteration , Loss 466 0.5811396241188049\n",
      "Iteration , Loss 467 0.5811344385147095\n",
      "Iteration , Loss 468 0.5811290740966797\n",
      "Iteration , Loss 469 0.5811238288879395\n",
      "Iteration , Loss 470 0.581118643283844\n",
      "Iteration , Loss 471 0.5811135172843933\n",
      "Iteration , Loss 472 0.5811084508895874\n",
      "Iteration , Loss 473 0.5811033844947815\n",
      "Iteration , Loss 474 0.5810982584953308\n",
      "Iteration , Loss 475 0.5810933709144592\n",
      "Iteration , Loss 476 0.5810884237289429\n",
      "Iteration , Loss 477 0.5810832977294922\n",
      "Iteration , Loss 478 0.5810786485671997\n",
      "Iteration , Loss 479 0.5810734629631042\n",
      "Iteration , Loss 480 0.5810686945915222\n",
      "Iteration , Loss 481 0.581063985824585\n",
      "Iteration , Loss 482 0.5810592174530029\n",
      "Iteration , Loss 483 0.5810545086860657\n",
      "Iteration , Loss 484 0.5810497999191284\n",
      "Iteration , Loss 485 0.5810449719429016\n",
      "Iteration , Loss 486 0.5810403823852539\n",
      "Iteration , Loss 487 0.5810357928276062\n",
      "Iteration , Loss 488 0.5810312628746033\n",
      "Iteration , Loss 489 0.5810267925262451\n",
      "Iteration , Loss 490 0.5810222625732422\n",
      "Iteration , Loss 491 0.5810177326202393\n",
      "Iteration , Loss 492 0.5810131430625916\n",
      "Iteration , Loss 493 0.5810087323188782\n",
      "Iteration , Loss 494 0.5810045003890991\n",
      "Iteration , Loss 495 0.5810000896453857\n",
      "Iteration , Loss 496 0.5809957385063171\n",
      "Iteration , Loss 497 0.5809915065765381\n",
      "Iteration , Loss 498 0.580987274646759\n",
      "Iteration , Loss 499 0.58098304271698\n",
      "Iteration , Loss 500 0.5809786915779114\n",
      "Iteration , Loss 501 0.5809747576713562\n",
      "Iteration , Loss 502 0.5809704661369324\n",
      "Iteration , Loss 503 0.5809662938117981\n",
      "Iteration , Loss 504 0.5809621810913086\n",
      "Iteration , Loss 505 0.5809581875801086\n",
      "Iteration , Loss 506 0.5809541344642639\n",
      "Iteration , Loss 507 0.5809500217437744\n",
      "Iteration , Loss 508 0.5809459686279297\n",
      "Iteration , Loss 509 0.5809418559074402\n",
      "Iteration , Loss 510 0.5809381008148193\n",
      "Iteration , Loss 511 0.5809342265129089\n",
      "Iteration , Loss 512 0.5809301137924194\n",
      "Iteration , Loss 513 0.5809264779090881\n",
      "Iteration , Loss 514 0.5809226632118225\n",
      "Iteration , Loss 515 0.5809189081192017\n",
      "Iteration , Loss 516 0.580915093421936\n",
      "Iteration , Loss 517 0.5809112191200256\n",
      "Iteration , Loss 518 0.5809074640274048\n",
      "Iteration , Loss 519 0.5809038281440735\n",
      "Iteration , Loss 520 0.5809001922607422\n",
      "Iteration , Loss 521 0.5808964967727661\n",
      "Iteration , Loss 522 0.5808929204940796\n",
      "Iteration , Loss 523 0.580889105796814\n",
      "Iteration , Loss 524 0.580885648727417\n",
      "Iteration , Loss 525 0.5808818340301514\n",
      "Iteration , Loss 526 0.5808784365653992\n",
      "Iteration , Loss 527 0.5808749794960022\n",
      "Iteration , Loss 528 0.5808715224266052\n",
      "Iteration , Loss 529 0.5808679461479187\n",
      "Iteration , Loss 530 0.5808644890785217\n",
      "Iteration , Loss 531 0.5808610320091248\n",
      "Iteration , Loss 532 0.5808576345443726\n",
      "Iteration , Loss 533 0.5808542966842651\n",
      "Iteration , Loss 534 0.5808508992195129\n",
      "Iteration , Loss 535 0.5808475017547607\n",
      "Iteration , Loss 536 0.5808442234992981\n",
      "Iteration , Loss 537 0.5808408856391907\n",
      "Iteration , Loss 538 0.5808377265930176\n",
      "Iteration , Loss 539 0.5808342099189758\n",
      "Iteration , Loss 540 0.5808309316635132\n",
      "Iteration , Loss 541 0.5808277130126953\n",
      "Iteration , Loss 542 0.5808244943618774\n",
      "Iteration , Loss 543 0.5808212161064148\n",
      "Iteration , Loss 544 0.5808181762695312\n",
      "Iteration , Loss 545 0.5808150768280029\n",
      "Iteration , Loss 546 0.5808117985725403\n",
      "Iteration , Loss 547 0.5808086395263672\n",
      "Iteration , Loss 548 0.5808055996894836\n",
      "Iteration , Loss 549 0.5808025598526001\n",
      "Iteration , Loss 550 0.580799400806427\n",
      "Iteration , Loss 551 0.5807963609695435\n",
      "Iteration , Loss 552 0.5807934403419495\n",
      "Iteration , Loss 553 0.5807903409004211\n",
      "Iteration , Loss 554 0.5807874798774719\n",
      "Iteration , Loss 555 0.5807844400405884\n",
      "Iteration , Loss 556 0.5807814002037048\n",
      "Iteration , Loss 557 0.5807785987854004\n",
      "Iteration , Loss 558 0.5807755589485168\n",
      "Iteration , Loss 559 0.5807728171348572\n",
      "Iteration , Loss 560 0.5807697772979736\n",
      "Iteration , Loss 561 0.5807669162750244\n",
      "Iteration , Loss 562 0.5807639956474304\n",
      "Iteration , Loss 563 0.5807612538337708\n",
      "Iteration , Loss 564 0.5807584524154663\n",
      "Iteration , Loss 565 0.5807555913925171\n",
      "Iteration , Loss 566 0.580752968788147\n",
      "Iteration , Loss 567 0.5807501077651978\n",
      "Iteration , Loss 568 0.5807474255561829\n",
      "Iteration , Loss 569 0.5807445645332336\n",
      "Iteration , Loss 570 0.5807417631149292\n",
      "Iteration , Loss 571 0.5807391405105591\n",
      "Iteration , Loss 572 0.5807363390922546\n",
      "Iteration , Loss 573 0.5807337164878845\n",
      "Iteration , Loss 574 0.5807309746742249\n",
      "Iteration , Loss 575 0.58072829246521\n",
      "Iteration , Loss 576 0.5807256102561951\n",
      "Iteration , Loss 577 0.5807230472564697\n",
      "Iteration , Loss 578 0.5807203054428101\n",
      "Iteration , Loss 579 0.5807178020477295\n",
      "Iteration , Loss 580 0.5807154178619385\n",
      "Iteration , Loss 581 0.5807127356529236\n",
      "Iteration , Loss 582 0.5807102918624878\n",
      "Iteration , Loss 583 0.580707848072052\n",
      "Iteration , Loss 584 0.5807052254676819\n",
      "Iteration , Loss 585 0.580702543258667\n",
      "Iteration , Loss 586 0.5807002782821655\n",
      "Iteration , Loss 587 0.580697774887085\n",
      "Iteration , Loss 588 0.5806953310966492\n",
      "Iteration , Loss 589 0.5806928873062134\n",
      "Iteration , Loss 590 0.5806905031204224\n",
      "Iteration , Loss 591 0.5806881189346313\n",
      "Iteration , Loss 592 0.5806857943534851\n",
      "Iteration , Loss 593 0.5806832313537598\n",
      "Iteration , Loss 594 0.5806809067726135\n",
      "Iteration , Loss 595 0.5806784629821777\n",
      "Iteration , Loss 596 0.5806761980056763\n",
      "Iteration , Loss 597 0.58067387342453\n",
      "Iteration , Loss 598 0.5806716680526733\n",
      "Iteration , Loss 599 0.5806692838668823\n",
      "Iteration , Loss 600 0.5806670784950256\n",
      "Iteration , Loss 601 0.5806646943092346\n",
      "Iteration , Loss 602 0.5806623697280884\n",
      "Iteration , Loss 603 0.5806601643562317\n",
      "Iteration , Loss 604 0.5806578993797302\n",
      "Iteration , Loss 605 0.5806557536125183\n",
      "Iteration , Loss 606 0.5806534886360168\n",
      "Iteration , Loss 607 0.5806512236595154\n",
      "Iteration , Loss 608 0.5806491374969482\n",
      "Iteration , Loss 609 0.5806466937065125\n",
      "Iteration , Loss 610 0.5806445479393005\n",
      "Iteration , Loss 611 0.5806425213813782\n",
      "Iteration , Loss 612 0.580640435218811\n",
      "Iteration , Loss 613 0.5806383490562439\n",
      "Iteration , Loss 614 0.5806360244750977\n",
      "Iteration , Loss 615 0.5806340575218201\n",
      "Iteration , Loss 616 0.5806318521499634\n",
      "Iteration , Loss 617 0.5806298851966858\n",
      "Iteration , Loss 618 0.5806276798248291\n",
      "Iteration , Loss 619 0.5806257128715515\n",
      "Iteration , Loss 620 0.5806235074996948\n",
      "Iteration , Loss 621 0.580621600151062\n",
      "Iteration , Loss 622 0.5806194543838501\n",
      "Iteration , Loss 623 0.5806174874305725\n",
      "Iteration , Loss 624 0.5806154608726501\n",
      "Iteration , Loss 625 0.580613374710083\n",
      "Iteration , Loss 626 0.5806113481521606\n",
      "Iteration , Loss 627 0.5806094408035278\n",
      "Iteration , Loss 628 0.5806073546409607\n",
      "Iteration , Loss 629 0.5806054472923279\n",
      "Iteration , Loss 630 0.5806034207344055\n",
      "Iteration , Loss 631 0.5806016325950623\n",
      "Iteration , Loss 632 0.5805996656417847\n",
      "Iteration , Loss 633 0.5805976986885071\n",
      "Iteration , Loss 634 0.5805957317352295\n",
      "Iteration , Loss 635 0.580594003200531\n",
      "Iteration , Loss 636 0.5805920362472534\n",
      "Iteration , Loss 637 0.580590009689331\n",
      "Iteration , Loss 638 0.580588161945343\n",
      "Iteration , Loss 639 0.5805862545967102\n",
      "Iteration , Loss 640 0.5805844068527222\n",
      "Iteration , Loss 641 0.5805825591087341\n",
      "Iteration , Loss 642 0.5805807709693909\n",
      "Iteration , Loss 643 0.5805789828300476\n",
      "Iteration , Loss 644 0.5805771350860596\n",
      "Iteration , Loss 645 0.5805752277374268\n",
      "Iteration , Loss 646 0.5805734992027283\n",
      "Iteration , Loss 647 0.580571711063385\n",
      "Iteration , Loss 648 0.5805700421333313\n",
      "Iteration , Loss 649 0.5805680751800537\n",
      "Iteration , Loss 650 0.5805662870407104\n",
      "Iteration , Loss 651 0.5805644989013672\n",
      "Iteration , Loss 652 0.580562949180603\n",
      "Iteration , Loss 653 0.5805612206459045\n",
      "Iteration , Loss 654 0.580559492111206\n",
      "Iteration , Loss 655 0.5805577635765076\n",
      "Iteration , Loss 656 0.5805559158325195\n",
      "Iteration , Loss 657 0.5805542469024658\n",
      "Iteration , Loss 658 0.5805525779724121\n",
      "Iteration , Loss 659 0.5805509090423584\n",
      "Iteration , Loss 660 0.5805492401123047\n",
      "Iteration , Loss 661 0.580547571182251\n",
      "Iteration , Loss 662 0.5805457830429077\n",
      "Iteration , Loss 663 0.5805439949035645\n",
      "Iteration , Loss 664 0.5805425047874451\n",
      "Iteration , Loss 665 0.5805408954620361\n",
      "Iteration , Loss 666 0.5805392265319824\n",
      "Iteration , Loss 667 0.580537736415863\n",
      "Iteration , Loss 668 0.580535888671875\n",
      "Iteration , Loss 669 0.5805343389511108\n",
      "Iteration , Loss 670 0.5805326700210571\n",
      "Iteration , Loss 671 0.5805311799049377\n",
      "Iteration , Loss 672 0.5805294513702393\n",
      "Iteration , Loss 673 0.5805279612541199\n",
      "Iteration , Loss 674 0.5805264711380005\n",
      "Iteration , Loss 675 0.5805248618125916\n",
      "Iteration , Loss 676 0.5805231332778931\n",
      "Iteration , Loss 677 0.5805217623710632\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration , Loss 678 0.5805200934410095\n",
      "Iteration , Loss 679 0.5805186629295349\n",
      "Iteration , Loss 680 0.5805171132087708\n",
      "Iteration , Loss 681 0.5805156230926514\n",
      "Iteration , Loss 682 0.5805139541625977\n",
      "Iteration , Loss 683 0.5805124640464783\n",
      "Iteration , Loss 684 0.5805111527442932\n",
      "Iteration , Loss 685 0.580509603023529\n",
      "Iteration , Loss 686 0.5805081129074097\n",
      "Iteration , Loss 687 0.5805066227912903\n",
      "Iteration , Loss 688 0.5805051922798157\n",
      "Iteration , Loss 689 0.5805038213729858\n",
      "Iteration , Loss 690 0.5805020928382874\n",
      "Iteration , Loss 691 0.5805006623268127\n",
      "Iteration , Loss 692 0.5804992318153381\n",
      "Iteration , Loss 693 0.5804978609085083\n",
      "Iteration , Loss 694 0.5804963707923889\n",
      "Iteration , Loss 695 0.5804949998855591\n",
      "Iteration , Loss 696 0.580493688583374\n",
      "Iteration , Loss 697 0.5804920196533203\n",
      "Iteration , Loss 698 0.5804906487464905\n",
      "Iteration , Loss 699 0.580489456653595\n"
     ]
    }
   ],
   "source": [
    "for t in range(700):\n",
    "    # Forward pass: Compute predicted y by passing x to the model\n",
    "    y_pred = model_torch(X_tensor)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = criterion(y_pred, Y_tensor)\n",
    "    print('Iteration , Loss',t, loss.data[0])\n",
    "\n",
    "    # Zero gradients, perform a backward pass, and update the weights.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final loss: Variable containing:\n",
      " 0.5805\n",
      "[torch.FloatTensor of size 1]\n",
      "\n",
      "torch.Size([20, 2])\n",
      "torch.Size([20])\n",
      "torch.Size([10, 20])\n",
      "torch.Size([10])\n",
      "torch.Size([3, 10])\n",
      "torch.Size([3])\n"
     ]
    }
   ],
   "source": [
    "loss = criterion(model_torch(X_tensor), Y_tensor)\n",
    "print('Final loss:',loss)\n",
    "\n",
    "for i in model_torch.parameters():\n",
    "    print(i.data.shape)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy = 0.967 %\n"
     ]
    }
   ],
   "source": [
    "y_pred_max = np.argmax(model_torch(X_tensor).data.numpy(),axis=1)\n",
    "acc = np.sum(y_pred_max == Y)/y_pred_max.shape[0]\n",
    "print('Accuracy = %.3f' % acc,'%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matriz de confusão:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>row_0</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0   0   1   2\n",
       "row_0            \n",
       "0      50   0   0\n",
       "1       0  47   1\n",
       "2       0   3  49"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Matriz de confusão:')\n",
    "import pandas as pd\n",
    "pd.crosstab(y_pred_max, Y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
